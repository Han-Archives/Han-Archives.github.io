{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ML/[ML] ensemble/",
    "result": {"data":{"cur":{"id":"3fa11651-8acf-5d81-bcdc-ad895235be35","html":"<h2 id=\"ensemble\" style=\"position:relative;\"><a href=\"#ensemble\" aria-label=\"ensemble permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ensemble</h2>\n<p>앙상블 기법이란 한마디로 쉽게 설명하자면 <strong>여러 전문가(ML)들이 협력하여 결론(예측)을 하는 방식</strong>.</p>\n<p><img src=\"/93ced71a7b96e5ecde96fd534997c98c/ens_1.png\" alt=\"ens_1.PNG\"></p>\n<h3 id=\"앙상블-학습-유형\" style=\"position:relative;\"><a href=\"#%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-%EC%9C%A0%ED%98%95\" aria-label=\"앙상블 학습 유형 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>앙상블 학습 유형</h3>\n<p>대표적인 3가지 학습 유형으로 <strong>배깅, 보팅, 부스팅 3가지 방법</strong>이 있다.</p>\n<h3 id=\"배깅bootstrap-aggregating-bagging\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85bootstrap-aggregating-bagging\" aria-label=\"배깅bootstrap aggregating bagging permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅(Bootstrap Aggregating, Bagging)</h3>\n<h4 id=\"배깅이란\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%B4%EB%9E%80\" aria-label=\"배깅이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅이란?</h4>\n<blockquote>\n<p>랜덤 샘플링한 데이터를 여러 모델에 학습시킨 뒤 결과를 집계하는 방식</p>\n</blockquote>\n<h4 id=\"특징\" style=\"position:relative;\"><a href=\"#%ED%8A%B9%EC%A7%95\" aria-label=\"특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>특징</h4>\n<ul>\n<li>\n<p>배깅의 경우 각 분류기가 서로 독립적으로 병렬적으로 진행</p>\n</li>\n<li>\n<p>범주형 자료일 때 다수결로 채택, 숫자형 자료일 때 평균 값을 채택</p>\n</li>\n<li>\n<p>속도가 빠르며 과적합 영향이 적다.</p>\n</li>\n<li>\n<p>적은 데이터셋이라도 준수한 결과를 도출한다.</p>\n</li>\n<li>\n<p>배깅의 대표적인 알고리즘 : RandomForest</p>\n</li>\n</ul>\n<h4 id=\"진행-과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\" aria-label=\"진행 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행 과정</h4>\n<p><img src=\"/5176a9f1d8def6595977e42df1978823/rf_3.png\" alt=\"rf_3.PNG\">\n[출처] <a href=\"https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f\">https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f</a></p>\n<h3 id=\"보팅\" style=\"position:relative;\"><a href=\"#%EB%B3%B4%ED%8C%85\" aria-label=\"보팅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>보팅</h3>\n<h4 id=\"보팅이란\" style=\"position:relative;\"><a href=\"#%EB%B3%B4%ED%8C%85%EC%9D%B4%EB%9E%80\" aria-label=\"보팅이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>보팅이란?</h4>\n<blockquote>\n<p>여러 분류기가 투표를 통해 예측 결과를 결정하는 방식</p>\n</blockquote>\n<h4 id=\"방식\" style=\"position:relative;\"><a href=\"#%EB%B0%A9%EC%8B%9D\" aria-label=\"방식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>방식</h4>\n<ul>\n<li>\n<p>소프트 보팅 :모든 분류기가 예측한 값의 결정 확률 평균을 구한 뒤 확률이 높은 값으로 결정</p>\n</li>\n<li>\n<p>하드 보팅 : 다수의 분류기가 예측한 값으로 결정</p>\n</li>\n</ul>\n<p><img src=\"/2c88b57bf183decb396a15286bbcc3b8/ens_2.png\" alt=\"ens_2.PNG\">\n[출처] <a href=\"https://velog.io/@gangjoo/ML-%EB%B6%84%EB%A5%98-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning%EA%B3%BC-%EB%B3%B4%ED%8C%85-Voting\">https://velog.io/@gangjoo/ML-%EB%B6%84%EB%A5%98-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning%EA%B3%BC-%EB%B3%B4%ED%8C%85-Voting</a></p>\n<h3 id=\"부스팅\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\" aria-label=\"부스팅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스팅</h3>\n<h4 id=\"부스팅이란\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85%EC%9D%B4%EB%9E%80\" aria-label=\"부스팅이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스팅이란?</h4>\n<blockquote>\n<p>부스팅은 가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법.</p>\n</blockquote>\n<p><img src=\"/24a7c14bcfbdc59571426527c9c16e8d/ens_4.png\" alt=\"ens_4.PNG\"></p>\n<h4 id=\"진행방식\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89%EB%B0%A9%EC%8B%9D\" aria-label=\"진행방식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행방식</h4>\n<ol>\n<li>한 라운드 당 하나의 모델을 학습</li>\n<li>각 라운드 당 오분류된 객체들의 가중치를 조절</li>\n<li>조절된 가중치로 다시 학습</li>\n</ol>\n<p>위 1~3 과정을 반복하여 결과</p>\n<h4 id=\"특징-1\" style=\"position:relative;\"><a href=\"#%ED%8A%B9%EC%A7%95-1\" aria-label=\"특징 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>특징</h4>\n<ul>\n<li>\n<p>각 분류기가 순차적으로 진행</p>\n</li>\n<li>\n<p>데이터셋에 과적화될 위험성이 큼</p>\n</li>\n<li>\n<p>배깅 방식에 비해 속도가 느림</p>\n</li>\n<li>\n<p>결과 도출시에도 각 모델 결과에 가중치를 반영한다. 쉽게 말하면 나중 모델의 결과에 더 높은 가중치를 둠.</p>\n</li>\n<li>\n<p>대표적인 알고리즘으로 XGBoost, GBM, LightBoost 등이 있다.</p>\n</li>\n</ul>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"http://www.dinnopartners.com/__trashed-4/\">http://www.dinnopartners.com/__trashed-4/</a></p>\n<p>[2] <a href=\"https://nicola-ml.tistory.com/95\">https://nicola-ml.tistory.com/95</a></p>\n<p>[3] <a href=\"https://blog.naver.com/PostView.naver?blogId=hajuny2903&#x26;logNo=222422472569&#x26;redirect=Dlog&#x26;widgetTypeCall=true&#x26;directAccess=false\">https://blog.naver.com/PostView.naver?blogId=hajuny2903&#x26;logNo=222422472569&#x26;redirect=Dlog&#x26;widgetTypeCall=true&#x26;directAccess=false</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#ensemble\">Ensemble</a></p>\n<ul>\n<li>\n<p><a href=\"#%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-%EC%9C%A0%ED%98%95\">앙상블 학습 유형</a></p>\n</li>\n<li>\n<p><a href=\"#%EB%B0%B0%EA%B9%85bootstrap-aggregating-bagging\">배깅(Bootstrap Aggregating, Bagging)</a></p>\n<ul>\n<li><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%B4%EB%9E%80\">배깅이란?</a></li>\n<li><a href=\"#%ED%8A%B9%EC%A7%95\">특징</a></li>\n<li><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\">진행 과정</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B3%B4%ED%8C%85\">보팅</a></p>\n<ul>\n<li><a href=\"#%EB%B3%B4%ED%8C%85%EC%9D%B4%EB%9E%80\">보팅이란?</a></li>\n<li><a href=\"#%EB%B0%A9%EC%8B%9D\">방식</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\">부스팅</a></p>\n<ul>\n<li><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85%EC%9D%B4%EB%9E%80\">부스팅이란?</a></li>\n<li><a href=\"#%EC%A7%84%ED%96%89%EB%B0%A9%EC%8B%9D\">진행방식</a></li>\n<li><a href=\"#%ED%8A%B9%EC%A7%95-1\">특징</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","excerpt":"Ensemble 앙상블 기법이란 한마디로 쉽게 설명하자면 여러 전문가(ML)들이 협력하여 결론(예측)을 하는 방식. ens_1.PNG 앙상블 학습 유형 대표적인 3가지 학습 유형으로 배깅, 보팅, 부스팅 3가지 방법이 있다. 배깅(Bootstrap Aggregating, Bagging) 배깅이란? 랜덤 샘플링한 데이터를 여러 모델에 학습시킨 뒤 결과를 집계하는 방식 특징 배깅의 경우 각 분류기가 서로 독립적으로 병렬적으로 진행 범주형 자료일 때 다수결로 채택, 숫자형 자료일 때 평균 값을 채택 속도가 빠르며 과적합 영향이 적다. 적은 데이터셋이라도 준수한 결과를 도출한다. 배깅의 대표적인 알고리즘 : RandomForest 진행 과정 rf_3.PNG\n[출처] https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f 보팅 보팅이란? 여러 분류기가 투표를 통해 …","frontmatter":{"date":"September 19, 2022","title":"Ensemble","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] ensemble/"}},"next":{"id":"b22a1546-b405-5a09-af07-080c0d044953","html":"<h3 id=\"keywords\" style=\"position:relative;\"><a href=\"#keywords\" aria-label=\"keywords permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Keywords</h3>\n<ul>\n<li>RandomForest</li>\n<li>배깅</li>\n<li>하이퍼 파라미터 튜닝</li>\n<li>OOB Score</li>\n<li>변수 중요도</li>\n</ul>\n<h2 id=\"randomforest\" style=\"position:relative;\"><a href=\"#randomforest\" aria-label=\"randomforest permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RandomForest</h2>\n<blockquote>\n<p>랜덤포레스트는 <strong>분류 및 회귀 ML 중 하나로 앙상블 학습 방법의 일종</strong> 으로 트리 기반 알고리즘이다. <strong>각 트리들은 랜덤하게 서로 다른 특성</strong>을 가진다. 이를 통해 각 트리들의 예측이 <strong>비상관적</strong>이며 <strong>결과적으로 일반화 성능을 향상</strong>시킨다.</p>\n</blockquote>\n<blockquote>\n<p>랜덤화는 각 트리들의 훈련 과정에서 진행되며, <strong>랜덤 학습 데이터 추출 방법을 이용한 앙상블 학습법인 배깅(bagging)과 랜덤 노드 최적화(randomized node optimization)가 자주 사용된다</strong>. 이 두 가지 방법은 서로 동시에 사용되어 랜덤화 특성을 더욱 증진 시킬 수 있다.</p>\n</blockquote>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.22222222222223%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAACTklEQVQ4y32TTU8TURSGu3TjxkT/AjtNICaEjSw0Gv0D7HDhwn8gcWFMjB/QosQoSMAEKEOkK0sLloS2JogYbRqjoIBYUCraDobSmc50OvfOY6YffFk9ybl5c/Ke95x7zr0eAMdxDriUjhtm8YfOjecpboXW8Ea+oRcF9fi1mGue3eA+F1XB+a852voXaH/6iav+JXKGvZdc9cPCnsPV9ne4Y9h8TGt82NBY/lXAFpJ6/AOC7uHmC7cgUBJgCbBlJb7f7CqvaDu7OS6vZq6oJ5lM4h9VmAyHicViPBsfZyIUIh6PEQgECE5MMDMzw9CIn2AwSDQaZWh4mKmpKaanI7tYiMp8PS759t0unvQPoCgKXl8PvX39KGNjdPnu87i3j/FAAF93DwODg2Xc6e1meMTPqKLQ6X1QxrZtV6/str01hzR+UsNOYbOCs3EcU4VSDqlGQVbmIzMRKBVACKQaA9vau7IuYVtfJ1fUyAnI6ets13Bhk23L4rdVIpNPkynBVgky+e+olkB1cW6FrCWp7MTB07FocCRkcPplgfNzeU5EDE7FdS681jj+osDJ6A4X5zSOTeo0xvNcmtc4OmnQ+kqnddbFJmdmNUxR3XIgbdH2VqNjwaD7S5H2hM41F6+YtCcNrq86PErDlQWbm59NHq5WOPeWTXwrJpcTGneWTMTes3Go70DRQK6lsFIp2NwAR9QeSF1+eYausDzkoroxNRzmTUMD75qaSDQ3U1TVcqqQ8q+c//+UmmAwSKKxkfdnz5FsacHKZiudSPnP//wHJwKqnS7X0FsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"rf_1.png\"\n        title=\"rf_1.png\"\n        src=\"/static/e671b4b0021826413ff8f13be067e3fb/37523/rf_1.png\"\n        srcset=\"/static/e671b4b0021826413ff8f13be067e3fb/e9ff0/rf_1.png 180w,\n/static/e671b4b0021826413ff8f13be067e3fb/f21e7/rf_1.png 360w,\n/static/e671b4b0021826413ff8f13be067e3fb/37523/rf_1.png 720w,\n/static/e671b4b0021826413ff8f13be067e3fb/c483d/rf_1.png 751w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</blockquote>\n<p>[출처] <a href=\"https://velog.io/@ayi4067/DAY27\">https://velog.io/@ayi4067/DAY27</a></p>\n<hr>\n<h3 id=\"배깅\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85\" aria-label=\"배깅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅</h3>\n<blockquote>\n<p>랜덤 샘플링한 데이터를 여러 모델에 학습시킨 뒤 결과를 집계하는 방식</p>\n</blockquote>\n<h4 id=\"배깅의-특징\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%98-%ED%8A%B9%EC%A7%95\" aria-label=\"배깅의 특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅의 특징</h4>\n<ul>\n<li>\n<p>배깅의 경우 각 분류기가 서로 독립적으로 병렬적으로 진행</p>\n</li>\n<li>\n<p>범주형 자료일 때 다수결로 채택, 숫자형 자료일 때 평균 값을 채택</p>\n</li>\n<li>\n<p>속도가 빠르며 과적합 영향이 적다.</p>\n</li>\n<li>\n<p>적은 데이터셋이라도 준수한 결과를 도출한다.</p>\n</li>\n<li>\n<p>배깅의 대표적인 알고리즘 : RandomForest</p>\n</li>\n</ul>\n<h4 id=\"부트스트랩-샘플링-방식\" style=\"position:relative;\"><a href=\"#%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9-%EC%83%98%ED%94%8C%EB%A7%81-%EB%B0%A9%EC%8B%9D\" aria-label=\"부트스트랩 샘플링 방식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부트스트랩 샘플링 방식</h4>\n<blockquote>\n<p>부트스트랩은 통계학 분야에서 <strong>여러 작은 데이터 셋을 임의로 생성하여 개별 평균의 분포도를 측정하는 목적을 위한 샘플링 방식</strong></p>\n</blockquote>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 628px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.666666666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABaUlEQVQY0yXOzW/TMABA8f7/Zw4ctjMXhASiE6UbIEE7NI3RqFXWVrB0az6WLHHiJI4T2+VNdLd3+umNAKrOsIj1/8QdwFhLKDr2uWKft/TVLVauGKRPUgZEYkeX/mXQCq17+r5Ha41zjpE2jvmmZPvb55C8w0oPbeHrKuF8+ciXVURzf4oOXtMEp1xuP/J9fcbTz/c0eUZRlpRliRAFxhhGvYVP6zXV9Qkkb8FJ9HDA2wluAsHiLkf6M/p4jHqc4IdXLPdXqGxC1xZUsqauJVVVvYA4w2Y95j49pwiniA6MGVjsyiN4HUgqf8YQjzHpB27DH6z2c1Q6QbcFsm5omgYp5QtoRETvTfkVfWP2J2MbNxycY7pMufBC2ss39LtX1HcnnC1yJl7JZy8nnY9RIkNU8ogJIRiGgdE/IBMVcRIh8hRnBox1xGVHXCi67AEr15h6Q1LUREIRFQ366YG+a2mVQil1vLTW8gxBALvyxOAFggAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"rf_2.png\"\n        title=\"rf_2.png\"\n        src=\"/static/3fdadd610fcb37feac9c216653aefded/3d84d/rf_2.png\"\n        srcset=\"/static/3fdadd610fcb37feac9c216653aefded/e9ff0/rf_2.png 180w,\n/static/3fdadd610fcb37feac9c216653aefded/f21e7/rf_2.png 360w,\n/static/3fdadd610fcb37feac9c216653aefded/3d84d/rf_2.png 628w\"\n        sizes=\"(max-width: 628px) 100vw, 628px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</blockquote>\n<p>[출처] <a href=\"https://velog.io/@ayi4067/DAY27\">https://velog.io/@ayi4067/DAY27</a></p>\n<h4 id=\"진행과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\" aria-label=\"진행과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행과정</h4>\n<blockquote>\n<p><img src=\"/5176a9f1d8def6595977e42df1978823/rf_3.png\" alt=\"rf_3.PNG\"></p>\n</blockquote>\n<p>[출처] <a href=\"https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f\">https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f</a></p>\n<hr>\n<h4 id=\"하이퍼-파라미터-튜닝\" style=\"position:relative;\"><a href=\"#%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D\" aria-label=\"하이퍼 파라미터 튜닝 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>하이퍼 파라미터 튜닝</h4>\n<blockquote>\n<p>n_estimators : 랜덤 포레스트의 <strong>결정 트리의 갯수</strong>를 지정. <em>default값은 10</em></p>\n</blockquote>\n<blockquote>\n<p>max_features : 최적의 분할을 위해 고려해야 할 <strong>특징들의 갯수</strong></p>\n</blockquote>\n<blockquote>\n<p>min_sample_split : 노드 분할을 위한 <strong>최소 샘플 데이터의 갯수</strong></p>\n</blockquote>\n<blockquote>\n<p>min_samples_leaf : 말단 노드가 되기 위한 최소한의 샘플 수</p>\n</blockquote>\n<blockquote>\n<p>max_depth : 트리의 최대 깊이</p>\n</blockquote>\n<blockquote>\n<p>max_leaf_nodes : 말단 노드의 최대 갯수</p>\n</blockquote>\n<h3 id=\"oob-스코어\" style=\"position:relative;\"><a href=\"#oob-%EC%8A%A4%EC%BD%94%EC%96%B4\" aria-label=\"oob 스코어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>OOB 스코어</h3>\n<h4 id=\"oob-오차\" style=\"position:relative;\"><a href=\"#oob-%EC%98%A4%EC%B0%A8\" aria-label=\"oob 오차 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>OOB 오차</h4>\n<blockquote>\n<p>일반적으로 배깅 방식은 데이터셋의 2/3을 훈련 데이터로 사용한다. 나머지 1/3을 OOB(Out-of-Bag)관측치라 한다. <strong>OOB와 훈련 데이터셋의 예측과의 차이를 OOB 오차</strong>라고 하며 이 오차가 <strong>배깅 방식에서 평가 방식</strong>으로 사용된다. 이 OOB 오차를 통해 <em>OOB 스코어를 계산</em>한다.</p>\n</blockquote>\n<h3 id=\"변수-중요도\" style=\"position:relative;\"><a href=\"#%EB%B3%80%EC%88%98-%EC%A4%91%EC%9A%94%EB%8F%84\" aria-label=\"변수 중요도 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>변수 중요도</h3>\n<blockquote>\n<p>랜덤 포레스트의 장점 중 하나로 <strong>변수의 중요도를 구할 수 있다.</strong> 변수의 중요도를 파악하는 방법은 <em>어떤 변수가 분할 변수로 사용되고 사용된 후의 불순도(오차제곱합 or 지니계수)가 많이 감소되는 크기를 구한 후 평균</em>을 통해 구한다.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/0e431b3e2c2f960e5e965894d094298d/rf_4.png\" alt=\"rf_4.PNG\"></p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></p>\n<p>[2] <a href=\"https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8\">https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8</a></p>\n<p>[3] <a href=\"https://medium.com/nerd-for-tech/random-forest-sturdy-algorithm-d60b9f9140d4\">https://medium.com/nerd-for-tech/random-forest-sturdy-algorithm-d60b9f9140d4</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<ul>\n<li><a href=\"#keywords\">Keywords</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#randomforest\">RandomForest</a></p>\n<ul>\n<li>\n<p><a href=\"#%EB%B0%B0%EA%B9%85\">배깅</a></p>\n<ul>\n<li><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%98-%ED%8A%B9%EC%A7%95\">배깅의 특징</a></li>\n<li><a href=\"#%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9-%EC%83%98%ED%94%8C%EB%A7%81-%EB%B0%A9%EC%8B%9D\">부트스트랩 샘플링 방식</a></li>\n<li><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\">진행과정</a></li>\n<li><a href=\"#%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D\">하이퍼 파라미터 튜닝</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#oob-%EC%8A%A4%EC%BD%94%EC%96%B4\">OOB 스코어</a></p>\n<ul>\n<li><a href=\"#oob-%EC%98%A4%EC%B0%A8\">OOB 오차</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B3%80%EC%88%98-%EC%A4%91%EC%9A%94%EB%8F%84\">변수 중요도</a></p>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"September 18, 2022","title":"RandomForest","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] RandomForest/"}},"prev":{"id":"b2ade22d-c2e0-5e32-b0ce-fb2a910908f8","html":"<h2 id=\"decision-trees\" style=\"position:relative;\"><a href=\"#decision-trees\" aria-label=\"decision trees permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Trees</h2>\n<p>Decision Tree(결정 트리)는 <em>지도 학습에서 분류 및 회귀에 사용되는 모델</em> 중 하나로 불순도가 낮아지는 방향으로 가지를 계속해서 분할.</p>\n<p><strong>ML 알고리즘 중에서 가장 직관적인 알고리즘</strong></p>\n<p><img src=\"/7c1dd10e6792b06c83464951abadf378/dt_main.png\" alt=\"dt_main.PNG\"></p>\n<h3 id=\"특징\" style=\"position:relative;\"><a href=\"#%ED%8A%B9%EC%A7%95\" aria-label=\"특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>특징</h3>\n<ul>\n<li>쉽게 이해할 수 있고 해석이 간편하다.</li>\n<li>별도의 전처리 없이 쉽게 사용이 가능하다.</li>\n<li>RandomForest 모형의 구성 요소</li>\n</ul>\n<h3 id=\"불순도\" style=\"position:relative;\"><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84\" aria-label=\"불순도 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>불순도</h3>\n<p><em>불순도란 다양한 요소들이 섞여있는 정도</em>를 의미. 대표적인 불순도 척도로 <strong>지니 계수</strong>와 <strong>엔트로피</strong>가 사용된다.</p>\n<h4 id=\"지니-계수\" style=\"position:relative;\"><a href=\"#%EC%A7%80%EB%8B%88-%EA%B3%84%EC%88%98\" aria-label=\"지니 계수 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>지니 계수</h4>\n<p>지니 계수란 경제적 불평등을 나타내는 용어로 <strong>0에 가까울 수록 평등하고 1에 가까울 수록 불평등</strong>을 나타낸다.</p>\n<p>의사 결정 트리에서의 지니계수는 이와 약간 달리 <strong>0.5값을 가질 때를 가장 불순도가 높다</strong>고 판단하며 지니계수가 0에 근접하도록 분할을 진행한다.</p>\n<p><strong>&#x3C;지니계수를 구하는 공식></strong>\n<img src=\"/f150531f7741a01be87673fa5b85de6a/dt_gini.png\" alt=\"dt_gini.PNG\"></p>\n<h4 id=\"엔트로피\" style=\"position:relative;\"><a href=\"#%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC\" aria-label=\"엔트로피 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>엔트로피</h4>\n<p>엔트로피란 정보이득을 나타내는 지표로 순도가 높을 때 얻는 정보 이득은 증가, 불순도가 높을수록 얻는 정보 이득은 감소. 지니 계수와 마찬가지로 엔트로피가 0에 가까울수록 좋으며 불순도가 낮아진다는 의미로 해석할 수 있다.</p>\n<p><strong>&#x3C;엔트로피를 구하는 공식></strong>\n<img src=\"/551fc7b929257a72f77b6b25ee5a07d2/dt_entropy.png\" alt=\"dt_entropy.PNG\"></p>\n<h4 id=\"불순도를-줄여나가는-과정\" style=\"position:relative;\"><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84%EB%A5%BC-%EC%A4%84%EC%97%AC%EB%82%98%EA%B0%80%EB%8A%94-%EA%B3%BC%EC%A0%95\" aria-label=\"불순도를 줄여나가는 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>불순도를 줄여나가는 과정</h4>\n<p><img src=\"/d160bb42a5bf9078537b95d8204c938a/dt_0.png\" alt=\"dt_0.PNG\"></p>\n<h3 id=\"decision-tree-알고리즘-유형\" style=\"position:relative;\"><a href=\"#decision-tree-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9C%A0%ED%98%95\" aria-label=\"decision tree 알고리즘 유형 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Tree 알고리즘 유형</h3>\n<ul>\n<li>ID3</li>\n</ul>\n<blockquote>\n<p>반복적으로 이분하는 알고리즘. 불순도로 엔트로피를 사용하며 독립변수가 범주형일 때만 사용가능. 연속형 독립변수 사용 불가.</p>\n</blockquote>\n<ul>\n<li>C4.5</li>\n</ul>\n<blockquote>\n<p>ID3의 단점을 보완한 알고리즘. 연속형 독립변수도 사용 가능</p>\n</blockquote>\n<ul>\n<li>CART (Classification And Regression Tree)</li>\n</ul>\n<blockquote>\n<p>가장 널리 사용되는 알고리즘. 분류 및 회귀 문제에서 사용 가능하며 분류 문제의 경우 불순도로 지니계수를, 회귀 문제의 경우 불순도로 분산을 사용하여 분류 진행.</p>\n</blockquote>\n<ul>\n<li>Chi-square (Chi-square automatic interaction detection)</li>\n</ul>\n<blockquote>\n<p>CART 알고리즘과 마찬가지로 분류 및 회귀 문제에서 사용 가능. 분류 문제의 경우 불순도로 카이제곱 검정값을, 회귀 문제의 경우 F 검정값으로 다지 분할을 진행</p>\n</blockquote>\n<h4 id=\"decision-tree-parameter\" style=\"position:relative;\"><a href=\"#decision-tree-parameter\" aria-label=\"decision tree parameter permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Tree Parameter</h4>\n<p>결정 트리 알고리즘에서 설정해야 할 파라미터는 다음과 같다.</p>\n<ul>\n<li><strong>min_sample_split</strong></li>\n</ul>\n<blockquote>\n<p>노드 분할을 위한 최소 샘플 데이터의 수</p>\n</blockquote>\n<ul>\n<li><strong>min_samples_leaf</strong></li>\n</ul>\n<blockquote>\n<p>말단 노드가 되기 위한 최소한의 샘플 수</p>\n</blockquote>\n<ul>\n<li><strong>max_features</strong></li>\n</ul>\n<blockquote>\n<p>최적의 분할을 위해 고려해야 할 특징들의 갯수</p>\n</blockquote>\n<ul>\n<li><strong>max_depth</strong></li>\n</ul>\n<blockquote>\n<p>트리의 최대 깊이</p>\n</blockquote>\n<ul>\n<li><strong>max_leaf_nodes</strong></li>\n</ul>\n<blockquote>\n<p>말단 노드의 최대 개수</p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://di-bigdata-study.tistory.com/2\">https://di-bigdata-study.tistory.com/2</a></p>\n<p>[2] <a href=\"https://scikit-learn.org/stable/modules/tree.html\">https://scikit-learn.org/stable/modules/tree.html</a></p>\n<p>[3] <a href=\"https://www.jcchouinard.com/decision-trees-in-machine-learning/\">https://www.jcchouinard.com/decision-trees-in-machine-learning/</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#decision-trees\">Decision Trees</a></p>\n<ul>\n<li>\n<p><a href=\"#%ED%8A%B9%EC%A7%95\">특징</a></p>\n</li>\n<li>\n<p><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84\">불순도</a></p>\n<ul>\n<li><a href=\"#%EC%A7%80%EB%8B%88-%EA%B3%84%EC%88%98\">지니 계수</a></li>\n<li><a href=\"#%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC\">엔트로피</a></li>\n<li><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84%EB%A5%BC-%EC%A4%84%EC%97%AC%EB%82%98%EA%B0%80%EB%8A%94-%EA%B3%BC%EC%A0%95\">불순도를 줄여나가는 과정</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#decision-tree-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9C%A0%ED%98%95\">Decision Tree 알고리즘 유형</a></p>\n<ul>\n<li><a href=\"#decision-tree-parameter\">Decision Tree Parameter</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"September 25, 2022","title":"Decision_Trees","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] Decision_Trees/"}},"site":{"siteMetadata":{"siteUrl":"https://han-archives.github.io","comments":{"utterances":{"repo":"Han-Archives/han-archives.github.io"}}}}},"pageContext":{"slug":"/ML/[ML] ensemble/","nextSlug":"/ML/[ML] RandomForest/","prevSlug":"/ML/[ML] Decision_Trees/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}