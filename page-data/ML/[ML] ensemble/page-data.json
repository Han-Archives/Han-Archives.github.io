{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ML/[ML] ensemble/",
    "result": {"data":{"cur":{"id":"3fa11651-8acf-5d81-bcdc-ad895235be35","html":"<h2 id=\"ensemble\" style=\"position:relative;\"><a href=\"#ensemble\" aria-label=\"ensemble permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ensemble</h2>\n<p>앙상블 기법이란 한마디로 쉽게 설명하자면 <strong>여러 전문가(ML)들이 협력하여 결론(예측)을 하는 방식</strong>.</p>\n<p><img src=\"/93ced71a7b96e5ecde96fd534997c98c/ens_1.png\" alt=\"ens_1.PNG\"></p>\n<h3 id=\"앙상블-학습-유형\" style=\"position:relative;\"><a href=\"#%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-%EC%9C%A0%ED%98%95\" aria-label=\"앙상블 학습 유형 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>앙상블 학습 유형</h3>\n<p>대표적인 3가지 학습 유형으로 <strong>배깅, 보팅, 부스팅 3가지 방법</strong>이 있다.</p>\n<h3 id=\"배깅bootstrap-aggregating-bagging\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85bootstrap-aggregating-bagging\" aria-label=\"배깅bootstrap aggregating bagging permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅(Bootstrap Aggregating, Bagging)</h3>\n<h4 id=\"배깅이란\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%B4%EB%9E%80\" aria-label=\"배깅이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅이란?</h4>\n<blockquote>\n<p>랜덤 샘플링한 데이터를 여러 모델에 학습시킨 뒤 결과를 집계하는 방식</p>\n</blockquote>\n<h4 id=\"특징\" style=\"position:relative;\"><a href=\"#%ED%8A%B9%EC%A7%95\" aria-label=\"특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>특징</h4>\n<ul>\n<li>\n<p>배깅의 경우 각 분류기가 서로 독립적으로 병렬적으로 진행</p>\n</li>\n<li>\n<p>범주형 자료일 때 다수결로 채택, 숫자형 자료일 때 평균 값을 채택</p>\n</li>\n<li>\n<p>속도가 빠르며 과적합 영향이 적다.</p>\n</li>\n<li>\n<p>적은 데이터셋이라도 준수한 결과를 도출한다.</p>\n</li>\n<li>\n<p>배깅의 대표적인 알고리즘 : RandomForest</p>\n</li>\n</ul>\n<h4 id=\"진행-과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\" aria-label=\"진행 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행 과정</h4>\n<p><img src=\"/5176a9f1d8def6595977e42df1978823/rf_3.png\" alt=\"rf_3.PNG\">\n[출처] <a href=\"https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f\">https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f</a></p>\n<h3 id=\"보팅\" style=\"position:relative;\"><a href=\"#%EB%B3%B4%ED%8C%85\" aria-label=\"보팅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>보팅</h3>\n<h4 id=\"보팅이란\" style=\"position:relative;\"><a href=\"#%EB%B3%B4%ED%8C%85%EC%9D%B4%EB%9E%80\" aria-label=\"보팅이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>보팅이란?</h4>\n<blockquote>\n<p>여러 분류기가 투표를 통해 예측 결과를 결정하는 방식</p>\n</blockquote>\n<h4 id=\"방식\" style=\"position:relative;\"><a href=\"#%EB%B0%A9%EC%8B%9D\" aria-label=\"방식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>방식</h4>\n<ul>\n<li>\n<p>소프트 보팅 :모든 분류기가 예측한 값의 결정 확률 평균을 구한 뒤 확률이 높은 값으로 결정</p>\n</li>\n<li>\n<p>하드 보팅 : 다수의 분류기가 예측한 값으로 결정</p>\n</li>\n</ul>\n<p><img src=\"/2c88b57bf183decb396a15286bbcc3b8/ens_2.png\" alt=\"ens_2.PNG\">\n[출처] <a href=\"https://velog.io/@gangjoo/ML-%EB%B6%84%EB%A5%98-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning%EA%B3%BC-%EB%B3%B4%ED%8C%85-Voting\">https://velog.io/@gangjoo/ML-%EB%B6%84%EB%A5%98-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning%EA%B3%BC-%EB%B3%B4%ED%8C%85-Voting</a></p>\n<h3 id=\"부스팅\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\" aria-label=\"부스팅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스팅</h3>\n<h4 id=\"부스팅이란\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85%EC%9D%B4%EB%9E%80\" aria-label=\"부스팅이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스팅이란?</h4>\n<blockquote>\n<p>부스팅은 가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법.</p>\n</blockquote>\n<p><img src=\"/24a7c14bcfbdc59571426527c9c16e8d/ens_4.png\" alt=\"ens_4.PNG\"></p>\n<h4 id=\"진행방식\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89%EB%B0%A9%EC%8B%9D\" aria-label=\"진행방식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행방식</h4>\n<ol>\n<li>한 라운드 당 하나의 모델을 학습</li>\n<li>각 라운드 당 오분류된 객체들의 가중치를 조절</li>\n<li>조절된 가중치로 다시 학습</li>\n</ol>\n<p>위 1~3 과정을 반복하여 결과</p>\n<h4 id=\"특징-1\" style=\"position:relative;\"><a href=\"#%ED%8A%B9%EC%A7%95-1\" aria-label=\"특징 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>특징</h4>\n<ul>\n<li>\n<p>각 분류기가 순차적으로 진행</p>\n</li>\n<li>\n<p>데이터셋에 과적화될 위험성이 큼</p>\n</li>\n<li>\n<p>배깅 방식에 비해 속도가 느림</p>\n</li>\n<li>\n<p>결과 도출시에도 각 모델 결과에 가중치를 반영한다. 쉽게 말하면 나중 모델의 결과에 더 높은 가중치를 둠.</p>\n</li>\n<li>\n<p>대표적인 알고리즘으로 XGBoost, GBM, LightBoost 등이 있다.</p>\n</li>\n</ul>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"http://www.dinnopartners.com/__trashed-4/\">http://www.dinnopartners.com/__trashed-4/</a></p>\n<p>[2] <a href=\"https://nicola-ml.tistory.com/95\">https://nicola-ml.tistory.com/95</a></p>\n<p>[3] <a href=\"https://blog.naver.com/PostView.naver?blogId=hajuny2903&#x26;logNo=222422472569&#x26;redirect=Dlog&#x26;widgetTypeCall=true&#x26;directAccess=false\">https://blog.naver.com/PostView.naver?blogId=hajuny2903&#x26;logNo=222422472569&#x26;redirect=Dlog&#x26;widgetTypeCall=true&#x26;directAccess=false</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#ensemble\">Ensemble</a></p>\n<ul>\n<li>\n<p><a href=\"#%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-%EC%9C%A0%ED%98%95\">앙상블 학습 유형</a></p>\n</li>\n<li>\n<p><a href=\"#%EB%B0%B0%EA%B9%85bootstrap-aggregating-bagging\">배깅(Bootstrap Aggregating, Bagging)</a></p>\n<ul>\n<li><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%B4%EB%9E%80\">배깅이란?</a></li>\n<li><a href=\"#%ED%8A%B9%EC%A7%95\">특징</a></li>\n<li><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\">진행 과정</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B3%B4%ED%8C%85\">보팅</a></p>\n<ul>\n<li><a href=\"#%EB%B3%B4%ED%8C%85%EC%9D%B4%EB%9E%80\">보팅이란?</a></li>\n<li><a href=\"#%EB%B0%A9%EC%8B%9D\">방식</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\">부스팅</a></p>\n<ul>\n<li><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85%EC%9D%B4%EB%9E%80\">부스팅이란?</a></li>\n<li><a href=\"#%EC%A7%84%ED%96%89%EB%B0%A9%EC%8B%9D\">진행방식</a></li>\n<li><a href=\"#%ED%8A%B9%EC%A7%95-1\">특징</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","excerpt":"Ensemble 앙상블 기법이란 한마디로 쉽게 설명하자면 여러 전문가(ML)들이 협력하여 결론(예측)을 하는 방식. ens_1.PNG 앙상블 학습 유형 대표적인 3가지 학습 유형으로 배깅, 보팅, 부스팅 3가지 방법이 있다. 배깅(Bootstrap Aggregating, Bagging) 배깅이란? 랜덤 샘플링한 데이터를 여러 모델에 학습시킨 뒤 결과를 집계하는 방식 특징 배깅의 경우 각 분류기가 서로 독립적으로 병렬적으로 진행 범주형 자료일 때 다수결로 채택, 숫자형 자료일 때 평균 값을 채택 속도가 빠르며 과적합 영향이 적다. 적은 데이터셋이라도 준수한 결과를 도출한다. 배깅의 대표적인 알고리즘 : RandomForest 진행 과정 rf_3.PNG\n[출처] https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f 보팅 보팅이란? 여러 분류기가 투표를 통해 …","frontmatter":{"date":"September 28, 2022","title":"Ensemble","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] ensemble/"}},"next":{"id":"faafee8b-fcf8-5f06-b2ac-3cbf460b6e14","html":"<h2 id=\"boosting-algorithm\" style=\"position:relative;\"><a href=\"#boosting-algorithm\" aria-label=\"boosting algorithm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Boosting Algorithm</h2>\n<h3 id=\"부스팅\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\" aria-label=\"부스팅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스팅</h3>\n<p>부스팅은 앙상블 학습 유형 중 하나로 <strong>약한 분류기를 순차적으로 학습-예측하면서 가중치를 조정하여 오류를 개선</strong>하면서 학습하는 방식.</p>\n<h4 id=\"대표적인-부스팅-머신러닝\" style=\"position:relative;\"><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%B6%80%EC%8A%A4%ED%8C%85-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D\" aria-label=\"대표적인 부스팅 머신러닝 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>대표적인 부스팅 머신러닝</h4>\n<blockquote>\n<p>AdaBoost</p>\n</blockquote>\n<blockquote>\n<p>GradientBoost</p>\n</blockquote>\n<blockquote>\n<p>XGBoost, LightGBM, CatBoost 등</p>\n</blockquote>\n<h3 id=\"adaboost\" style=\"position:relative;\"><a href=\"#adaboost\" aria-label=\"adaboost permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AdaBoost</h3>\n<blockquote>\n<p><strong>예측 성능이 낮은 학습기를 구축 및 조합하여 가중치 조절을 통해 좋은 성능을 발휘하는 강한 분류기를 합성하는 알고리즘</strong></p>\n</blockquote>\n<h4 id=\"진행-과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\" aria-label=\"진행 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행 과정</h4>\n<blockquote>\n<p><img src=\"/8aeb4d0b52fe1469868424cb8102fa68/bs_1.png\" alt=\"bs_1.PNG\"></p>\n</blockquote>\n<h4 id=\"정리\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EB%A6%AC\" aria-label=\"정리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정리</h4>\n<p>AdaBoost는 매 단계마다 이전 분류기에서 오차가 크거나 오분류된 데이터들의 가중치를 크게하고 정분류된 데이터들의 가중치는 적게 설정한뒤 다음 단계의 학습데이터셋의 추출과정에 가중치에 비례하게 복원추출하여 새로운 데이터셋을 만들고 모형을 적합하는 과정을 거친다.</p>\n<p>이러한 반복 단계를 통해 가중치가 반영된 총 오류를 최소화하는 분류기를 선택하고, 선택된 분류기에서 얻은 가중치 및 오류를 얻고, 이를 가속화된 분류기를 개선하는 데 이용한다.</p>\n<hr>\n<h3 id=\"gradient-boost\" style=\"position:relative;\"><a href=\"#gradient-boost\" aria-label=\"gradient boost permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Gradient Boost</h3>\n<blockquote>\n<p>AdaBoost와 마찬가지로 예측력이 낮은 분류기를 토대로 반복 학습하여 강한 분류기를 생성하는 것. AdaBoost와 달리 반복과정을 통해 <strong>손실함수의 최소값을 찾는 과정으로 진행</strong>한다.</p>\n</blockquote>\n<blockquote>\n<p>Gradient Boost는 XGBoost, LightBoost 등의 토대가 되는 알고리즘</p>\n</blockquote>\n<blockquote>\n<p><strong>알아야할 용어</strong></p>\n</blockquote>\n<blockquote>\n<blockquote>\n<p><img src=\"/ca3da52336dbdb7ea578339a3676384e/bs_2.png\" alt=\"bs_2.PNG\"></p>\n</blockquote>\n</blockquote>\n<h4 id=\"진행과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\" aria-label=\"진행과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행과정</h4>\n<blockquote>\n<p><img src=\"/623dcce697912e872a7778026408b262/bs_3.png\" alt=\"bs_3.PNG\"></p>\n</blockquote>\n<blockquote>\n<p>좀 더 구체적으로 이해를 돕자면 손실함수가 RSS일때</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/afedd2baf462e0db5a992d2fe2bb28aa/bs_4.png\" alt=\"bs_4.PNG\"></p>\n</blockquote>\n<p>[출처] <a href=\"https://m.blog.naver.com/luvwithcat/222103025023\">https://m.blog.naver.com/luvwithcat/222103025023</a></p>\n<blockquote>\n<p>위와 같이 잔차를 획득하고 잔차에 대한 예측을 시행한 후 나온 결정트리의 (맨 아래 leaf의 값 중 하나 * 학습률)을 잔차와 더해 잔차의 값을 최소화하는 방향으로 진행.</p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-14-AdaBoost\">https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-14-AdaBoost</a></p>\n<p>[2] <a href=\"https://zephyrus1111.tistory.com/195\">https://zephyrus1111.tistory.com/195</a></p>\n<p>[3] <a href=\"https://www.youtube.com/watch?v=3CC4N4z3GJc\">https://www.youtube.com/watch?v=3CC4N4z3GJc</a></p>\n<p>[4] <a href=\"https://zephyrus1111.tistory.com/232?category=858748\">https://zephyrus1111.tistory.com/232?category=858748</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#boosting-algorithm\">Boosting Algorithm</a></p>\n<ul>\n<li>\n<p><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\">부스팅</a></p>\n<ul>\n<li><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%B6%80%EC%8A%A4%ED%8C%85-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D\">대표적인 부스팅 머신러닝</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#adaboost\">AdaBoost</a></p>\n<ul>\n<li><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\">진행 과정</a></li>\n<li><a href=\"#%EC%A0%95%EB%A6%AC\">정리</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#gradient-boost\">Gradient Boost</a></p>\n<ul>\n<li><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\">진행과정</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"September 25, 2022","title":"Boosting Alogrithms","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] Boosting_Algorithm/"}},"prev":{"id":"16192a93-09e0-5aae-8051-61a4d4e9991f","html":"<h2 id=\"xgboost\" style=\"position:relative;\"><a href=\"#xgboost\" aria-label=\"xgboost permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>XGBoost</h2>\n<h3 id=\"소개\" style=\"position:relative;\"><a href=\"#%EC%86%8C%EA%B0%9C\" aria-label=\"소개 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>소개</h3>\n<blockquote>\n<p>GBM을 최적화한 알고리즘.</p>\n</blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 709px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 54.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAEDBAX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHWjtMYw//EABoQAQADAAMAAAAAAAAAAAAAAAEAAhEDIjH/2gAIAQEAAQUC5HDtVPLVLCDMyf/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABoQAAICAwAAAAAAAAAAAAAAAAABAhAREiH/2gAIAQEABj8CFtLKddQq/8QAGRABAQEBAQEAAAAAAAAAAAAAAREAMSGx/9oACAEBAAE/IXZJVh7h9ok5NfX3QgJlEnOYAgTf/9oADAMBAAIAAwAAABB4D//EABcRAAMBAAAAAAAAAAAAAAAAAAEQMSH/2gAIAQMBAT8QF2L/xAAXEQEAAwAAAAAAAAAAAAAAAAABEBEh/9oACAECAQE/ELQyP//EABwQAQACAgMBAAAAAAAAAAAAAAEAESFRQWGBof/aAAgBAQABPxDD4jjaXlINtYBTF04mIEE1b7FDginsImrvpKohon//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"xgb_1.jpg\"\n        title=\"xgb_1.jpg\"\n        src=\"/static/b8630edc224d36b0f5c8f95dc40814ef/bd958/xgb_1.jpg\"\n        srcset=\"/static/b8630edc224d36b0f5c8f95dc40814ef/4ec73/xgb_1.jpg 180w,\n/static/b8630edc224d36b0f5c8f95dc40814ef/158ba/xgb_1.jpg 360w,\n/static/b8630edc224d36b0f5c8f95dc40814ef/bd958/xgb_1.jpg 709w\"\n        sizes=\"(max-width: 709px) 100vw, 709px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<blockquote>\n<p>XGBoost 핵심 라이브러리는 C/C++로 작성되어 더 빠른 수행을 지원하며, 병렬처리 지원, 과적합 방지 등 여러 기능을 추가한 알고리즘</p>\n</blockquote>\n<h3 id=\"장단점\" style=\"position:relative;\"><a href=\"#%EC%9E%A5%EB%8B%A8%EC%A0%90\" aria-label=\"장단점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>장단점</h3>\n<h4 id=\"장점\" style=\"position:relative;\"><a href=\"#%EC%9E%A5%EC%A0%90\" aria-label=\"장점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>장점</h4>\n<ul>\n<li>뛰어난 예측 성능</li>\n<li>빠른 수행 시간 (GBM 대비)</li>\n<li>과적합 규제</li>\n<li>Tree Pruning</li>\n<li>자체 내장된 교차 검증 기능</li>\n</ul>\n<p>-자체적으로 결손값 처리</p>\n<h4 id=\"단점\" style=\"position:relative;\"><a href=\"#%EB%8B%A8%EC%A0%90\" aria-label=\"단점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>단점</h4>\n<ul>\n<li>데이터가 크기가 작은 경우 과적합이 될 가능성이 크다.</li>\n</ul>\n<hr>\n<h3 id=\"xgboost-알고리즘\" style=\"position:relative;\"><a href=\"#xgboost-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98\" aria-label=\"xgboost 알고리즘 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>XGBoost 알고리즘</h3>\n<blockquote>\n<p><img src=\"/a1a0cf7518567b458c25b2b22fef7a1a/xgb_2.png\" alt=\"xgb_2.PNG\"></p>\n</blockquote>\n<blockquote>\n<p><img src=\"/99dfe06d9260e8be780a1e35f0bdbed7/xgb_3.png\" alt=\"xgb_3.PNG\"></p>\n</blockquote>\n<blockquote>\n<p><img src=\"/ea0e3db66e72ec294d6c56b9f2ae1fba/xgb_4.png\" alt=\"xgb_4.PNG\"></p>\n</blockquote>\n<blockquote>\n<p><img src=\"/60da6b7743bd5387b346ec60c726a117/xgb_5.png\" alt=\"xgb_5.PNG\"></p>\n</blockquote>\n<h3 id=\"xgboost-hyper-parameter\" style=\"position:relative;\"><a href=\"#xgboost-hyper-parameter\" aria-label=\"xgboost hyper parameter permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>XGBoost Hyper Parameter</h3>\n<h4 id=\"일반-파라미터\" style=\"position:relative;\"><a href=\"#%EC%9D%BC%EB%B0%98-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0\" aria-label=\"일반 파라미터 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>일반 파라미터</h4>\n<blockquote>\n<p>booster : gbtree or gblinear 선택. default는 gbtree</p>\n</blockquote>\n<blockquote>\n<p>silent: 출력 메시지를 나타내고 싶으면 0, 아니면 1. default는 0</p>\n</blockquote>\n<blockquote>\n<p>nthread: CPU의 실행 Thread 수를 조정. default는 CPU의 전체 스레드 사용</p>\n</blockquote>\n<h4 id=\"부스터-파라미터\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%84%B0-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0\" aria-label=\"부스터 파라미터 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스터 파라미터</h4>\n<blockquote>\n<p>eta : learning rate로 사이킷런 기반이면 deault는 0.1, 0.01~0.2 사이의 값을 선호</p>\n</blockquote>\n<blockquote>\n<p>num_boost_rounds : n_estimators 와 같은 의미</p>\n</blockquote>\n<blockquote>\n<p>min_child_weight: 추가로 가지를 나눌것인지 결정하기 위한 필요 데이터들의 가중치의 합. min_child_weight이 클수록 분할을 자제함. 과적합 조절을 위해 사용. default는 1</p>\n</blockquote>\n<blockquote>\n<p>max_depth : 최대 깊이 설정. 0이면 깊이에 제한이 없고 max_depth가 크면 과적합이 높아짐. default는 6</p>\n</blockquote>\n<blockquote>\n<p>max_leaf_node : 트리의 최대 리프 노드의 수를 설정</p>\n</blockquote>\n<blockquote>\n<p>gamma : 트리의 리프 노드를 추가로 나눌지 결정할 최소 손실 감소 값. 값이 클수록 과적합 감소. default는 0.</p>\n</blockquote>\n<blockquote>\n<p>subsample : 트리가 커져서 과적합 되는 제어하기 위해 데이터를 샘플링하는 비율. 일반적으로 0.5~1 사이 값을 사용. default는 1.</p>\n</blockquote>\n<blockquote>\n<p>colsample_bytree: 트리 생성에 필요한 칼럼을 임의로 샘플링하는데 사용. 많은 칼럼이 있는 경우 과적합 조정을 위해 사용.</p>\n</blockquote>\n<blockquote>\n<p>lambda : L2 Regularization 적용 값. feature의 수가 많은 경우 적용을 검토. 값이 클수록 과적합 감소. default=1</p>\n</blockquote>\n<blockquote>\n<p>alpha : L1 Regularization 적용 값. feature의 수가 많은 경우 적용 검토. 값이 클수록 과적합감소. default = 0</p>\n</blockquote>\n<blockquote>\n<p>scale_pos_weight : 특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 셋의 균형을 유지하기 위한 파라미터. default는 1.</p>\n</blockquote>\n<h4 id=\"학습-태스크-파라미터\" style=\"position:relative;\"><a href=\"#%ED%95%99%EC%8A%B5-%ED%83%9C%EC%8A%A4%ED%81%AC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0\" aria-label=\"학습 태스크 파라미터 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>학습 태스크 파라미터</h4>\n<blockquote>\n<p>objective : 최솟값을 가져할 손실함수를 정의. 많이 사용되는 손실함수로</p>\n<blockquote>\n<p>reg:linear : 회귀일 때 사용</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>reg:logistic : 분류일 때 사용</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>binary:logistic : 이진 분류일 때 사용</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>multi:softmax : 다중 분류일때 사용</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>multi:softprob : softmax와 유사하지만 개별 레이블 클래스의 해당되는 예측 확률을 반환</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<p>eval_metric: 검증에 사용되는 함수. defualt로 회귀인 경우 rmse, 분류의 경우 error. eval_metric 유형으로</p>\n<blockquote>\n<p>rmse : Root Mean Square Error</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>mae : Mean Absolute Error</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>logloss : Negative log-Likelihood</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>error – Binary classification error rate (0.5 threshold)</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>merror – Multiclass classification error rate</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>mlogloss – Multiclass logloss</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>auc – Area under the curve</p>\n</blockquote>\n</blockquote>\n<h3 id=\"참고-사항\" style=\"position:relative;\"><a href=\"#%EC%B0%B8%EA%B3%A0-%EC%82%AC%ED%95%AD\" aria-label=\"참고 사항 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>참고 사항</h3>\n<h4 id=\"자체적으로-gridsearch와-유사한-교차-검증-수행-api를-제공\" style=\"position:relative;\"><a href=\"#%EC%9E%90%EC%B2%B4%EC%A0%81%EC%9C%BC%EB%A1%9C-gridsearch%EC%99%80-%EC%9C%A0%EC%82%AC%ED%95%9C-%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9D-%EC%88%98%ED%96%89-api%EB%A5%BC-%EC%A0%9C%EA%B3%B5\" aria-label=\"자체적으로 gridsearch와 유사한 교차 검증 수행 api를 제공 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>자체적으로 GridSearch와 유사한 교차 검증 수행 API를 제공</h4>\n<blockquote>\n<p>xgboost.cv()</p>\n<blockquote>\n<p>params(dict): 부스터 파라미터</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>dtrain(DMatrix): 학습 데이터</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>num_boost_round(int): 부스팅 반복 횟수</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>nfold(int): cv fold 수</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>stratified(bool) : cv 수행 시 층화표본추출 수행여부</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>metrics (string or list of strings) : cv 수행 시 모니터링할 성능 평가 지표</p>\n</blockquote>\n</blockquote>\n<blockquote>\n<blockquote>\n<p>early_stopping_rounds (int) : 조기 중단을 활성화시킴. 반복 횟수 지정</p>\n</blockquote>\n</blockquote>\n<h4 id=\"randomforest-처럼-feature-importance-plot-제공\" style=\"position:relative;\"><a href=\"#randomforest-%EC%B2%98%EB%9F%BC-feature-importance-plot-%EC%A0%9C%EA%B3%B5\" aria-label=\"randomforest 처럼 feature importance plot 제공 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RandomForest 처럼 Feature importance plot 제공</h4>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.77777777777778%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAAB00lEQVQ4y62RT0sbQRTAh1JsoQjiwUiqFy8KWvwCBaH0FlIK2qhfoZfevQheSj9AG0V2e7FoQVSEehBs/piDiZLdjWgSVjeUuGvXbqpsZtVm980rs4Zii9ik9MGPefPevH/zyMiLyAMxtrc2Gy9KYqKYFuLFjJBoHDFRzEQ38lvxfT1vn51OEEICHVMr8veF7TLOJg9RSGkobDaOmCrhu9gBJlULa5cXIiH3ugOv5tK69o0iIrqICP/ADx5cpXSakLsPO8ejSaNkOdwGNQ/QhSbxwAWG6FA6QwhpC67slIxL13eABwyvwxjjhX6dNwljjE+G1E/Y3tM5Fk0an3LHWG//pgC8TX5PSO4HJpckI1Y4weXsEazKOnKWs0e4kTf9LpvqsHdgMPjkzfrxyw9ZfJ/SgG9sJnGAbz+rOJ/+0nzCvv6BrqHX68aqYvynke8Egh+3NP30vIYAjG+ZuXU8ANagXP/DtuDO4derjQDzeME60IDu3xljNR7vOA5P2NqxmNFOuMHzPH88APD5U/+bz7ZtgTwdetyyt6uEKaVh0zSfKYoyWqlUwrZth2RZHtV1/TmlNKSq6nChUBjhOn8nSdKYZVnharUayuVykXK5HDFN89FPcKGNnACACyAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"xgb_6.png\"\n        title=\"xgb_6.png\"\n        src=\"/static/a0e8f69c89678d9a573612ff0289dcb1/37523/xgb_6.png\"\n        srcset=\"/static/a0e8f69c89678d9a573612ff0289dcb1/e9ff0/xgb_6.png 180w,\n/static/a0e8f69c89678d9a573612ff0289dcb1/f21e7/xgb_6.png 360w,\n/static/a0e8f69c89678d9a573612ff0289dcb1/37523/xgb_6.png 720w,\n/static/a0e8f69c89678d9a573612ff0289dcb1/5bb8b/xgb_6.png 749w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</blockquote>\n<p>[출처] <a href=\"https://mljar.com/blog/feature-importance-xgboost/\">https://mljar.com/blog/feature-importance-xgboost/</a></p>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://www.youtube.com/watch?v=VkaZXGknN3g\">https://www.youtube.com/watch?v=VkaZXGknN3g</a></p>\n<p>[2] <a href=\"https://www.youtube.com/watch?v=OtD8wVaFm6E&#x26;list=WL&#x26;index=2\">https://www.youtube.com/watch?v=OtD8wVaFm6E&#x26;list=WL&#x26;index=2</a></p>\n<p>[3] <a href=\"https://zephyrus1111.tistory.com/232\">https://zephyrus1111.tistory.com/232</a></p>\n<p>[4] <a href=\"https://www.kaggle.com/code/azminetoushikwasi/xgboost-wrangling-with-hyperparameters-guide\">https://www.kaggle.com/code/azminetoushikwasi/xgboost-wrangling-with-hyperparameters-guide</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#xgboost\">XGBoost</a></p>\n<ul>\n<li>\n<p><a href=\"#%EC%86%8C%EA%B0%9C\">소개</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%9E%A5%EB%8B%A8%EC%A0%90\">장단점</a></p>\n<ul>\n<li><a href=\"#%EC%9E%A5%EC%A0%90\">장점</a></li>\n<li><a href=\"#%EB%8B%A8%EC%A0%90\">단점</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#xgboost-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98\">XGBoost 알고리즘</a></p>\n</li>\n<li>\n<p><a href=\"#xgboost-hyper-parameter\">XGBoost Hyper Parameter</a></p>\n<ul>\n<li><a href=\"#%EC%9D%BC%EB%B0%98-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0\">일반 파라미터</a></li>\n<li><a href=\"#%EB%B6%80%EC%8A%A4%ED%84%B0-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0\">부스터 파라미터</a></li>\n<li><a href=\"#%ED%95%99%EC%8A%B5-%ED%83%9C%EC%8A%A4%ED%81%AC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0\">학습 태스크 파라미터</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EC%B0%B8%EA%B3%A0-%EC%82%AC%ED%95%AD\">참고 사항</a></p>\n<ul>\n<li><a href=\"#%EC%9E%90%EC%B2%B4%EC%A0%81%EC%9C%BC%EB%A1%9C-gridsearch%EC%99%80-%EC%9C%A0%EC%82%AC%ED%95%9C-%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9D-%EC%88%98%ED%96%89-api%EB%A5%BC-%EC%A0%9C%EA%B3%B5\">자체적으로 GridSearch와 유사한 교차 검증 수행 API를 제공</a></li>\n<li><a href=\"#randomforest-%EC%B2%98%EB%9F%BC-feature-importance-plot-%EC%A0%9C%EA%B3%B5\">RandomForest 처럼 Feature importance plot 제공</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"October 13, 2022","title":"XGBoost","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] XGBoost/"}},"site":{"siteMetadata":{"siteUrl":"https://han-archives.github.io","comments":{"utterances":{"repo":"Han-Archives/han-archives.github.io"}}}}},"pageContext":{"slug":"/ML/[ML] ensemble/","nextSlug":"/ML/[ML] Boosting_Algorithm/","prevSlug":"/ML/[ML] XGBoost/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}