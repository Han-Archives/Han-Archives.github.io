{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ML/[ML] RandomForest/",
    "result": {"data":{"cur":{"id":"b22a1546-b405-5a09-af07-080c0d044953","html":"<h3 id=\"keywords\" style=\"position:relative;\"><a href=\"#keywords\" aria-label=\"keywords permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Keywords</h3>\n<ul>\n<li>RandomForest</li>\n<li>배깅</li>\n<li>하이퍼 파라미터 튜닝</li>\n<li>OOB Score</li>\n<li>변수 중요도</li>\n</ul>\n<h2 id=\"randomforest\" style=\"position:relative;\"><a href=\"#randomforest\" aria-label=\"randomforest permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RandomForest</h2>\n<blockquote>\n<p>랜덤포레스트는 <strong>분류 및 회귀 ML 중 하나로 앙상블 학습 방법의 일종</strong> 으로 트리 기반 알고리즘이다. <strong>각 트리들은 랜덤하게 서로 다른 특성</strong>을 가진다. 이를 통해 각 트리들의 예측이 <strong>비상관적</strong>이며 <strong>결과적으로 일반화 성능을 향상</strong>시킨다.</p>\n</blockquote>\n<blockquote>\n<p>랜덤화는 각 트리들의 훈련 과정에서 진행되며, <strong>랜덤 학습 데이터 추출 방법을 이용한 앙상블 학습법인 배깅(bagging)과 랜덤 노드 최적화(randomized node optimization)가 자주 사용된다</strong>. 이 두 가지 방법은 서로 동시에 사용되어 랜덤화 특성을 더욱 증진 시킬 수 있다.</p>\n</blockquote>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.22222222222223%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAACTklEQVQ4y32TTU8TURSGu3TjxkT/AjtNICaEjSw0Gv0D7HDhwn8gcWFMjB/QosQoSMAEKEOkK0sLloS2JogYbRqjoIBYUCraDobSmc50OvfOY6YffFk9ybl5c/Ke95x7zr0eAMdxDriUjhtm8YfOjecpboXW8Ea+oRcF9fi1mGue3eA+F1XB+a852voXaH/6iav+JXKGvZdc9cPCnsPV9ne4Y9h8TGt82NBY/lXAFpJ6/AOC7uHmC7cgUBJgCbBlJb7f7CqvaDu7OS6vZq6oJ5lM4h9VmAyHicViPBsfZyIUIh6PEQgECE5MMDMzw9CIn2AwSDQaZWh4mKmpKaanI7tYiMp8PS759t0unvQPoCgKXl8PvX39KGNjdPnu87i3j/FAAF93DwODg2Xc6e1meMTPqKLQ6X1QxrZtV6/str01hzR+UsNOYbOCs3EcU4VSDqlGQVbmIzMRKBVACKQaA9vau7IuYVtfJ1fUyAnI6ets13Bhk23L4rdVIpNPkynBVgky+e+olkB1cW6FrCWp7MTB07FocCRkcPplgfNzeU5EDE7FdS681jj+osDJ6A4X5zSOTeo0xvNcmtc4OmnQ+kqnddbFJmdmNUxR3XIgbdH2VqNjwaD7S5H2hM41F6+YtCcNrq86PErDlQWbm59NHq5WOPeWTXwrJpcTGneWTMTes3Go70DRQK6lsFIp2NwAR9QeSF1+eYausDzkoroxNRzmTUMD75qaSDQ3U1TVcqqQ8q+c//+UmmAwSKKxkfdnz5FsacHKZiudSPnP//wHJwKqnS7X0FsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"rf_1.png\"\n        title=\"rf_1.png\"\n        src=\"/static/e671b4b0021826413ff8f13be067e3fb/37523/rf_1.png\"\n        srcset=\"/static/e671b4b0021826413ff8f13be067e3fb/e9ff0/rf_1.png 180w,\n/static/e671b4b0021826413ff8f13be067e3fb/f21e7/rf_1.png 360w,\n/static/e671b4b0021826413ff8f13be067e3fb/37523/rf_1.png 720w,\n/static/e671b4b0021826413ff8f13be067e3fb/c483d/rf_1.png 751w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</blockquote>\n<p>[출처] <a href=\"https://velog.io/@ayi4067/DAY27\">https://velog.io/@ayi4067/DAY27</a></p>\n<hr>\n<h3 id=\"배깅\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85\" aria-label=\"배깅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅</h3>\n<blockquote>\n<p>랜덤 샘플링한 데이터를 여러 모델에 학습시킨 뒤 결과를 집계하는 방식</p>\n</blockquote>\n<h4 id=\"배깅의-특징\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%98-%ED%8A%B9%EC%A7%95\" aria-label=\"배깅의 특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅의 특징</h4>\n<ul>\n<li>\n<p>배깅의 경우 각 분류기가 서로 독립적으로 병렬적으로 진행</p>\n</li>\n<li>\n<p>범주형 자료일 때 다수결로 채택, 숫자형 자료일 때 평균 값을 채택</p>\n</li>\n<li>\n<p>속도가 빠르며 과적합 영향이 적다.</p>\n</li>\n<li>\n<p>적은 데이터셋이라도 준수한 결과를 도출한다.</p>\n</li>\n<li>\n<p>배깅의 대표적인 알고리즘 : RandomForest</p>\n</li>\n</ul>\n<h4 id=\"부트스트랩-샘플링-방식\" style=\"position:relative;\"><a href=\"#%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9-%EC%83%98%ED%94%8C%EB%A7%81-%EB%B0%A9%EC%8B%9D\" aria-label=\"부트스트랩 샘플링 방식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부트스트랩 샘플링 방식</h4>\n<blockquote>\n<p>부트스트랩은 통계학 분야에서 <strong>여러 작은 데이터 셋을 임의로 생성하여 개별 평균의 분포도를 측정하는 목적을 위한 샘플링 방식</strong></p>\n</blockquote>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 628px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.666666666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABaUlEQVQY0yXOzW/TMABA8f7/Zw4ctjMXhASiE6UbIEE7NI3RqFXWVrB0az6WLHHiJI4T2+VNdLd3+umNAKrOsIj1/8QdwFhLKDr2uWKft/TVLVauGKRPUgZEYkeX/mXQCq17+r5Ha41zjpE2jvmmZPvb55C8w0oPbeHrKuF8+ciXVURzf4oOXtMEp1xuP/J9fcbTz/c0eUZRlpRliRAFxhhGvYVP6zXV9Qkkb8FJ9HDA2wluAsHiLkf6M/p4jHqc4IdXLPdXqGxC1xZUsqauJVVVvYA4w2Y95j49pwiniA6MGVjsyiN4HUgqf8YQjzHpB27DH6z2c1Q6QbcFsm5omgYp5QtoRETvTfkVfWP2J2MbNxycY7pMufBC2ss39LtX1HcnnC1yJl7JZy8nnY9RIkNU8ogJIRiGgdE/IBMVcRIh8hRnBox1xGVHXCi67AEr15h6Q1LUREIRFQ366YG+a2mVQil1vLTW8gxBALvyxOAFggAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"rf_2.png\"\n        title=\"rf_2.png\"\n        src=\"/static/3fdadd610fcb37feac9c216653aefded/3d84d/rf_2.png\"\n        srcset=\"/static/3fdadd610fcb37feac9c216653aefded/e9ff0/rf_2.png 180w,\n/static/3fdadd610fcb37feac9c216653aefded/f21e7/rf_2.png 360w,\n/static/3fdadd610fcb37feac9c216653aefded/3d84d/rf_2.png 628w\"\n        sizes=\"(max-width: 628px) 100vw, 628px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</blockquote>\n<p>[출처] <a href=\"https://velog.io/@ayi4067/DAY27\">https://velog.io/@ayi4067/DAY27</a></p>\n<h4 id=\"진행과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\" aria-label=\"진행과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행과정</h4>\n<blockquote>\n<p><img src=\"/5176a9f1d8def6595977e42df1978823/rf_3.png\" alt=\"rf_3.PNG\"></p>\n</blockquote>\n<p>[출처] <a href=\"https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f\">https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f</a></p>\n<hr>\n<h4 id=\"하이퍼-파라미터-튜닝\" style=\"position:relative;\"><a href=\"#%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D\" aria-label=\"하이퍼 파라미터 튜닝 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>하이퍼 파라미터 튜닝</h4>\n<blockquote>\n<p>n_estimators : 랜덤 포레스트의 <strong>결정 트리의 갯수</strong>를 지정. <em>default값은 10</em></p>\n</blockquote>\n<blockquote>\n<p>max_features : 최적의 분할을 위해 고려해야 할 <strong>특징들의 갯수</strong></p>\n</blockquote>\n<blockquote>\n<p>min_sample_split : 노드 분할을 위한 <strong>최소 샘플 데이터의 갯수</strong></p>\n</blockquote>\n<blockquote>\n<p>min_samples_leaf : 말단 노드가 되기 위한 최소한의 샘플 수</p>\n</blockquote>\n<blockquote>\n<p>max_depth : 트리의 최대 깊이</p>\n</blockquote>\n<blockquote>\n<p>max_leaf_nodes : 말단 노드의 최대 갯수</p>\n</blockquote>\n<h3 id=\"oob-스코어\" style=\"position:relative;\"><a href=\"#oob-%EC%8A%A4%EC%BD%94%EC%96%B4\" aria-label=\"oob 스코어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>OOB 스코어</h3>\n<h4 id=\"oob-오차\" style=\"position:relative;\"><a href=\"#oob-%EC%98%A4%EC%B0%A8\" aria-label=\"oob 오차 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>OOB 오차</h4>\n<blockquote>\n<p>일반적으로 배깅 방식은 데이터셋의 2/3을 훈련 데이터로 사용한다. 나머지 1/3을 OOB(Out-of-Bag)관측치라 한다. <strong>OOB와 훈련 데이터셋의 예측과의 차이를 OOB 오차</strong>라고 하며 이 오차가 <strong>배깅 방식에서 평가 방식</strong>으로 사용된다. 이 OOB 오차를 통해 <em>OOB 스코어를 계산</em>한다.</p>\n</blockquote>\n<h3 id=\"변수-중요도\" style=\"position:relative;\"><a href=\"#%EB%B3%80%EC%88%98-%EC%A4%91%EC%9A%94%EB%8F%84\" aria-label=\"변수 중요도 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>변수 중요도</h3>\n<blockquote>\n<p>랜덤 포레스트의 장점 중 하나로 <strong>변수의 중요도를 구할 수 있다.</strong> 변수의 중요도를 파악하는 방법은 <em>어떤 변수가 분할 변수로 사용되고 사용된 후의 불순도(오차제곱합 or 지니계수)가 많이 감소되는 크기를 구한 후 평균</em>을 통해 구한다.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/0e431b3e2c2f960e5e965894d094298d/rf_4.png\" alt=\"rf_4.PNG\"></p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></p>\n<p>[2] <a href=\"https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8\">https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8</a></p>\n<p>[3] <a href=\"https://medium.com/nerd-for-tech/random-forest-sturdy-algorithm-d60b9f9140d4\">https://medium.com/nerd-for-tech/random-forest-sturdy-algorithm-d60b9f9140d4</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<ul>\n<li><a href=\"#keywords\">Keywords</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#randomforest\">RandomForest</a></p>\n<ul>\n<li>\n<p><a href=\"#%EB%B0%B0%EA%B9%85\">배깅</a></p>\n<ul>\n<li><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%98-%ED%8A%B9%EC%A7%95\">배깅의 특징</a></li>\n<li><a href=\"#%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9-%EC%83%98%ED%94%8C%EB%A7%81-%EB%B0%A9%EC%8B%9D\">부트스트랩 샘플링 방식</a></li>\n<li><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\">진행과정</a></li>\n<li><a href=\"#%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D\">하이퍼 파라미터 튜닝</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#oob-%EC%8A%A4%EC%BD%94%EC%96%B4\">OOB 스코어</a></p>\n<ul>\n<li><a href=\"#oob-%EC%98%A4%EC%B0%A8\">OOB 오차</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B3%80%EC%88%98-%EC%A4%91%EC%9A%94%EB%8F%84\">변수 중요도</a></p>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","excerpt":"Keywords RandomForest 배깅 하이퍼 파라미터 튜닝 OOB Score 변수 중요도 RandomForest 랜덤포레스트는 분류 및 회귀 ML 중 하나로 앙상블 학습 방법의 일종 으로 트리 기반 알고리즘이다. 각 트리들은 랜덤하게 서로 다른 특성을 가진다. 이를 통해 각 트리들의 예측이 비상관적이며 결과적으로 일반화 성능을 향상시킨다. 랜덤화는 각 트리들의 훈련 과정에서 진행되며, 랜덤 학습 데이터 추출 방법을 이용한 앙상블 학습법인 배깅(bagging)과 랜덤 노드 최적화(randomized node optimization)가 자주 사용된다. 이 두 가지 방법은 서로 동시에 사용되어 랜덤화 특성을 더욱 증진 시킬 수 있다.  [출처] https://velog.io/@ayi4067/DAY27 배깅 랜덤 샘플링한 데이터를 여러 모델에 학습시킨 뒤 결과를 집계하는 방식 배깅의 특징 배깅의 경우 각 분류기가 서로 독립적으로 병렬적으로 진행 범주형 자료일 때 다수결로 채택, 숫…","frontmatter":{"date":"September 18, 2022","title":"RandomForest","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] RandomForest/"}},"next":{"id":"849eb3fd-8385-55b6-af45-ca82ffb6acc6","html":"<h2 id=\"svm\" style=\"position:relative;\"><a href=\"#svm\" aria-label=\"svm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SVM</h2>\n<h3 id=\"소개\" style=\"position:relative;\"><a href=\"#%EC%86%8C%EA%B0%9C\" aria-label=\"소개 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>소개</h3>\n<p>SVM (Support Vector Machine)은 지도학습 모델 중 하나로 분류 및 회귀 분석에 사용되는 머신 러닝 기법</p>\n<p><img src=\"/65d97514013db4043faee8610b9194ef/svm_1.png\" alt=\"svm_1.PNG\"></p>\n<h3 id=\"주요-용어\" style=\"position:relative;\"><a href=\"#%EC%A3%BC%EC%9A%94-%EC%9A%A9%EC%96%B4\" aria-label=\"주요 용어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>주요 용어</h3>\n<p><img src=\"/0c98c7f17fb14118f1649eac1d3d5562/svm_2.png\" alt=\"svm_2.PNG\"></p>\n<h3 id=\"경계선을-정하는-기준\" style=\"position:relative;\"><a href=\"#%EA%B2%BD%EA%B3%84%EC%84%A0%EC%9D%84-%EC%A0%95%ED%95%98%EB%8A%94-%EA%B8%B0%EC%A4%80\" aria-label=\"경계선을 정하는 기준 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>경계선을 정하는 기준</h3>\n<p>아래 그림처럼 다양한 경계선이 생성될 수 있다. 어떠한 경계선을 선택하는 것이 옳을까?\n<img src=\"/743f9331cc388008dedb5183b3f13bf9/svm_3.png\" alt=\"svm_3.PNG\"></p>\n<p>margin이 크다는 의미는 경계선의 폭이 크다는 의미로 새로운 데이터 값이 들어오는 경우에 margin의 클 수록 더 정확한 분류가 가능</p>\n<h3 id=\"커널-트릭\" style=\"position:relative;\"><a href=\"#%EC%BB%A4%EB%84%90-%ED%8A%B8%EB%A6%AD\" aria-label=\"커널 트릭 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>커널 트릭</h3>\n<p>커널 트릭이란 저차원 -> 고차원 공간으로 매핑하는 작업을 말한다. 아래 그림과 같이 경계선을 생성 불가능 할때 주로 사용.\n<img src=\"/d4392ce275cc5360e861fcfb920389b9/svm_4.png\" alt=\"svm_4.PNG\"></p>\n<h3 id=\"주요-파라미터\" style=\"position:relative;\"><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0\" aria-label=\"주요 파라미터 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>주요 파라미터</h3>\n<ul>\n<li>C</li>\n</ul>\n<blockquote>\n<p>C는 오분류된 데이터의 허용 정도를 의미하며 이 값이 작을수록 많이 허용하고 높을 수록 적게 허용한다. 이로 인해 C값이 작으면 경계선 모양이 직선에 가까우며 높을 수록 여러겹의 굽은 선에 가깝다.</p>\n</blockquote>\n<ul>\n<li>Gamma</li>\n</ul>\n<blockquote>\n<p>각 데이터가 영향을 주는 정도를 나타내며 Gamma 값이 낮아지면 각 데이터의 영향력이 커져 큰 범위를 형성하는 반면 Gamma 값이 높아지면 각 데이터의 영향력이 적어져 좁은 범위를 형성한다.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/d672f117b8c19f50c199f1fba85721c4/svm_5.png\" alt=\"svm_5.PNG\"></p>\n</blockquote>\n<p>출처 : <a href=\"https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\">https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html</a></p>\n<ul>\n<li>Kernel 함수</li>\n</ul>\n<blockquote>\n<p>커널 함수는 데이터를 필요한 형식으로 변환하는 함수로 SVM에서는 선형, 비선형, RBF, 시그모이드 등의 여러 커널 함수가 사용된다. 주로 사용되는 커널 함수는 RBF(가우스 방사형 기저함수)로 데이터에 대한 사전 지식없이 사용 가능하며 앞서 언급한 Kernel 트릭에서 커널 함수로 매핑한 데이터를 분류하는데 사용.</p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://www.youtube.com/watch?v=efR1C6CvhmE\">https://www.youtube.com/watch?v=efR1C6CvhmE</a></p>\n<p>[2] <a href=\"https://scikit-learn.org/stable/modules/svm.html\">https://scikit-learn.org/stable/modules/svm.html</a></p>\n<p>[3] <a href=\"https://bskyvision.com/entry/%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0SVM%EC%9D%98-%EC%82%AC%EC%9A%A9%EC%9E%90%EB%A1%9C%EC%84%9C-%EA%BC%AD-%EC%95%8C%EC%95%84%EC%95%BC%ED%95%A0-%EA%B2%83%EB%93%A4-%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98-C%EC%99%80-gamma\">https://bskyvision.com/entry/%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0SVM%EC%9D%98-%EC%82%AC%EC%9A%A9%EC%9E%90%EB%A1%9C%EC%84%9C-%EA%BC%AD-%EC%95%8C%EC%95%84%EC%95%BC%ED%95%A0-%EA%B2%83%EB%93%A4-%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98-C%EC%99%80-gamma</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#svm\">SVM</a></p>\n<ul>\n<li><a href=\"#%EC%86%8C%EA%B0%9C\">소개</a></li>\n<li><a href=\"#%EC%A3%BC%EC%9A%94-%EC%9A%A9%EC%96%B4\">주요 용어</a></li>\n<li><a href=\"#%EA%B2%BD%EA%B3%84%EC%84%A0%EC%9D%84-%EC%A0%95%ED%95%98%EB%8A%94-%EA%B8%B0%EC%A4%80\">경계선을 정하는 기준</a></li>\n<li><a href=\"#%EC%BB%A4%EB%84%90-%ED%8A%B8%EB%A6%AD\">커널 트릭</a></li>\n<li><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0\">주요 파라미터</a></li>\n<li><a href=\"#reference\">Reference</a></li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"September 17, 2022","title":"SVM","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] SVM/"}},"prev":{"id":"b2ade22d-c2e0-5e32-b0ce-fb2a910908f8","html":"<h2 id=\"decision-trees\" style=\"position:relative;\"><a href=\"#decision-trees\" aria-label=\"decision trees permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Trees</h2>\n<p>Decision Tree(결정 트리)는 <em>지도 학습에서 분류 및 회귀에 사용되는 모델</em> 중 하나로 불순도가 낮아지는 방향으로 가지를 계속해서 분할.</p>\n<p><strong>ML 알고리즘 중에서 가장 직관적인 알고리즘</strong></p>\n<p><img src=\"/7c1dd10e6792b06c83464951abadf378/dt_main.png\" alt=\"dt_main.PNG\"></p>\n<h3 id=\"특징\" style=\"position:relative;\"><a href=\"#%ED%8A%B9%EC%A7%95\" aria-label=\"특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>특징</h3>\n<ul>\n<li>쉽게 이해할 수 있고 해석이 간편하다.</li>\n<li>별도의 전처리 없이 쉽게 사용이 가능하다.</li>\n<li>RandomForest 모형의 구성 요소</li>\n</ul>\n<h3 id=\"불순도\" style=\"position:relative;\"><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84\" aria-label=\"불순도 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>불순도</h3>\n<p><em>불순도란 다양한 요소들이 섞여있는 정도</em>를 의미. 대표적인 불순도 척도로 <strong>지니 계수</strong>와 <strong>엔트로피</strong>가 사용된다.</p>\n<h4 id=\"지니-계수\" style=\"position:relative;\"><a href=\"#%EC%A7%80%EB%8B%88-%EA%B3%84%EC%88%98\" aria-label=\"지니 계수 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>지니 계수</h4>\n<p>지니 계수란 경제적 불평등을 나타내는 용어로 <strong>0에 가까울 수록 평등하고 1에 가까울 수록 불평등</strong>을 나타낸다.</p>\n<p>의사 결정 트리에서의 지니계수는 이와 약간 달리 <strong>0.5값을 가질 때를 가장 불순도가 높다</strong>고 판단하며 지니계수가 0에 근접하도록 분할을 진행한다.</p>\n<p><strong>&#x3C;지니계수를 구하는 공식></strong>\n<img src=\"/f150531f7741a01be87673fa5b85de6a/dt_gini.png\" alt=\"dt_gini.PNG\"></p>\n<h4 id=\"엔트로피\" style=\"position:relative;\"><a href=\"#%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC\" aria-label=\"엔트로피 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>엔트로피</h4>\n<p>엔트로피란 정보이득을 나타내는 지표로 순도가 높을 때 얻는 정보 이득은 증가, 불순도가 높을수록 얻는 정보 이득은 감소. 지니 계수와 마찬가지로 엔트로피가 0에 가까울수록 좋으며 불순도가 낮아진다는 의미로 해석할 수 있다.</p>\n<p><strong>&#x3C;엔트로피를 구하는 공식></strong>\n<img src=\"/551fc7b929257a72f77b6b25ee5a07d2/dt_entropy.png\" alt=\"dt_entropy.PNG\"></p>\n<h4 id=\"불순도를-줄여나가는-과정\" style=\"position:relative;\"><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84%EB%A5%BC-%EC%A4%84%EC%97%AC%EB%82%98%EA%B0%80%EB%8A%94-%EA%B3%BC%EC%A0%95\" aria-label=\"불순도를 줄여나가는 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>불순도를 줄여나가는 과정</h4>\n<p><img src=\"/d160bb42a5bf9078537b95d8204c938a/dt_0.png\" alt=\"dt_0.PNG\"></p>\n<h3 id=\"decision-tree-알고리즘-유형\" style=\"position:relative;\"><a href=\"#decision-tree-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9C%A0%ED%98%95\" aria-label=\"decision tree 알고리즘 유형 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Tree 알고리즘 유형</h3>\n<ul>\n<li>ID3</li>\n</ul>\n<blockquote>\n<p>반복적으로 이분하는 알고리즘. 불순도로 엔트로피를 사용하며 독립변수가 범주형일 때만 사용가능. 연속형 독립변수 사용 불가.</p>\n</blockquote>\n<ul>\n<li>C4.5</li>\n</ul>\n<blockquote>\n<p>ID3의 단점을 보완한 알고리즘. 연속형 독립변수도 사용 가능</p>\n</blockquote>\n<ul>\n<li>CART (Classification And Regression Tree)</li>\n</ul>\n<blockquote>\n<p>가장 널리 사용되는 알고리즘. 분류 및 회귀 문제에서 사용 가능하며 분류 문제의 경우 불순도로 지니계수를, 회귀 문제의 경우 불순도로 분산을 사용하여 분류 진행.</p>\n</blockquote>\n<ul>\n<li>Chi-square (Chi-square automatic interaction detection)</li>\n</ul>\n<blockquote>\n<p>CART 알고리즘과 마찬가지로 분류 및 회귀 문제에서 사용 가능. 분류 문제의 경우 불순도로 카이제곱 검정값을, 회귀 문제의 경우 F 검정값으로 다지 분할을 진행</p>\n</blockquote>\n<h4 id=\"decision-tree-parameter\" style=\"position:relative;\"><a href=\"#decision-tree-parameter\" aria-label=\"decision tree parameter permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Tree Parameter</h4>\n<p>결정 트리 알고리즘에서 설정해야 할 파라미터는 다음과 같다.</p>\n<ul>\n<li><strong>min_sample_split</strong></li>\n</ul>\n<blockquote>\n<p>노드 분할을 위한 최소 샘플 데이터의 수</p>\n</blockquote>\n<ul>\n<li><strong>min_samples_leaf</strong></li>\n</ul>\n<blockquote>\n<p>말단 노드가 되기 위한 최소한의 샘플 수</p>\n</blockquote>\n<ul>\n<li><strong>max_features</strong></li>\n</ul>\n<blockquote>\n<p>최적의 분할을 위해 고려해야 할 특징들의 갯수</p>\n</blockquote>\n<ul>\n<li><strong>max_depth</strong></li>\n</ul>\n<blockquote>\n<p>트리의 최대 깊이</p>\n</blockquote>\n<ul>\n<li><strong>max_leaf_nodes</strong></li>\n</ul>\n<blockquote>\n<p>말단 노드의 최대 개수</p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://di-bigdata-study.tistory.com/2\">https://di-bigdata-study.tistory.com/2</a></p>\n<p>[2] <a href=\"https://scikit-learn.org/stable/modules/tree.html\">https://scikit-learn.org/stable/modules/tree.html</a></p>\n<p>[3] <a href=\"https://www.jcchouinard.com/decision-trees-in-machine-learning/\">https://www.jcchouinard.com/decision-trees-in-machine-learning/</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#decision-trees\">Decision Trees</a></p>\n<ul>\n<li>\n<p><a href=\"#%ED%8A%B9%EC%A7%95\">특징</a></p>\n</li>\n<li>\n<p><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84\">불순도</a></p>\n<ul>\n<li><a href=\"#%EC%A7%80%EB%8B%88-%EA%B3%84%EC%88%98\">지니 계수</a></li>\n<li><a href=\"#%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC\">엔트로피</a></li>\n<li><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84%EB%A5%BC-%EC%A4%84%EC%97%AC%EB%82%98%EA%B0%80%EB%8A%94-%EA%B3%BC%EC%A0%95\">불순도를 줄여나가는 과정</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#decision-tree-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9C%A0%ED%98%95\">Decision Tree 알고리즘 유형</a></p>\n<ul>\n<li><a href=\"#decision-tree-parameter\">Decision Tree Parameter</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"September 25, 2022","title":"Decision_Trees","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] Decision_Trees/"}},"site":{"siteMetadata":{"siteUrl":"https://han-archives.github.io","comments":{"utterances":{"repo":"Han-Archives/han-archives.github.io"}}}}},"pageContext":{"slug":"/ML/[ML] RandomForest/","nextSlug":"/ML/[ML] SVM/","prevSlug":"/ML/[ML] Decision_Trees/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}