{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ML/[ML] RandomForest/",
    "result": {"data":{"cur":{"id":"b22a1546-b405-5a09-af07-080c0d044953","html":"<h3 id=\"keywords\" style=\"position:relative;\"><a href=\"#keywords\" aria-label=\"keywords permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Keywords</h3>\n<ul>\n<li>RandomForest</li>\n<li>배깅</li>\n<li>하이퍼 파라미터 튜닝</li>\n<li>OOB Score</li>\n<li>변수 중요도</li>\n</ul>\n<h2 id=\"randomforest\" style=\"position:relative;\"><a href=\"#randomforest\" aria-label=\"randomforest permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RandomForest</h2>\n<blockquote>\n<p>랜덤포레스트는 <strong>분류 및 회귀 ML 중 하나로 앙상블 학습 방법의 일종</strong> 으로 트리 기반 알고리즘이다. <strong>각 트리들은 랜덤하게 서로 다른 특성</strong>을 가진다. 이를 통해 각 트리들의 예측이 <strong>비상관적</strong>이며 <strong>결과적으로 일반화 성능을 향상</strong>시킨다.</p>\n</blockquote>\n<blockquote>\n<p>랜덤화는 각 트리들의 훈련 과정에서 진행되며, <strong>랜덤 학습 데이터 추출 방법을 이용한 앙상블 학습법인 배깅(bagging)과 랜덤 노드 최적화(randomized node optimization)가 자주 사용된다</strong>. 이 두 가지 방법은 서로 동시에 사용되어 랜덤화 특성을 더욱 증진 시킬 수 있다.</p>\n</blockquote>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.22222222222223%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAACTklEQVQ4y32TTU8TURSGu3TjxkT/AjtNICaEjSw0Gv0D7HDhwn8gcWFMjB/QosQoSMAEKEOkK0sLloS2JogYbRqjoIBYUCraDobSmc50OvfOY6YffFk9ybl5c/Ke95x7zr0eAMdxDriUjhtm8YfOjecpboXW8Ea+oRcF9fi1mGue3eA+F1XB+a852voXaH/6iav+JXKGvZdc9cPCnsPV9ne4Y9h8TGt82NBY/lXAFpJ6/AOC7uHmC7cgUBJgCbBlJb7f7CqvaDu7OS6vZq6oJ5lM4h9VmAyHicViPBsfZyIUIh6PEQgECE5MMDMzw9CIn2AwSDQaZWh4mKmpKaanI7tYiMp8PS759t0unvQPoCgKXl8PvX39KGNjdPnu87i3j/FAAF93DwODg2Xc6e1meMTPqKLQ6X1QxrZtV6/str01hzR+UsNOYbOCs3EcU4VSDqlGQVbmIzMRKBVACKQaA9vau7IuYVtfJ1fUyAnI6ets13Bhk23L4rdVIpNPkynBVgky+e+olkB1cW6FrCWp7MTB07FocCRkcPplgfNzeU5EDE7FdS681jj+osDJ6A4X5zSOTeo0xvNcmtc4OmnQ+kqnddbFJmdmNUxR3XIgbdH2VqNjwaD7S5H2hM41F6+YtCcNrq86PErDlQWbm59NHq5WOPeWTXwrJpcTGneWTMTes3Go70DRQK6lsFIp2NwAR9QeSF1+eYausDzkoroxNRzmTUMD75qaSDQ3U1TVcqqQ8q+c//+UmmAwSKKxkfdnz5FsacHKZiudSPnP//wHJwKqnS7X0FsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"rf_1.png\"\n        title=\"rf_1.png\"\n        src=\"/static/e671b4b0021826413ff8f13be067e3fb/37523/rf_1.png\"\n        srcset=\"/static/e671b4b0021826413ff8f13be067e3fb/e9ff0/rf_1.png 180w,\n/static/e671b4b0021826413ff8f13be067e3fb/f21e7/rf_1.png 360w,\n/static/e671b4b0021826413ff8f13be067e3fb/37523/rf_1.png 720w,\n/static/e671b4b0021826413ff8f13be067e3fb/c483d/rf_1.png 751w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</blockquote>\n<p>[출처] <a href=\"https://velog.io/@ayi4067/DAY27\">https://velog.io/@ayi4067/DAY27</a></p>\n<hr>\n<h3 id=\"배깅\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85\" aria-label=\"배깅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅</h3>\n<blockquote>\n<p>랜덤 샘플링한 데이터를 여러 모델에 학습시킨 뒤 결과를 집계하는 방식</p>\n</blockquote>\n<h4 id=\"배깅의-특징\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%98-%ED%8A%B9%EC%A7%95\" aria-label=\"배깅의 특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅의 특징</h4>\n<ul>\n<li>\n<p>배깅의 경우 각 분류기가 서로 독립적으로 병렬적으로 진행</p>\n</li>\n<li>\n<p>범주형 자료일 때 다수결로 채택, 숫자형 자료일 때 평균 값을 채택</p>\n</li>\n<li>\n<p>속도가 빠르며 과적합 영향이 적다.</p>\n</li>\n<li>\n<p>적은 데이터셋이라도 준수한 결과를 도출한다.</p>\n</li>\n<li>\n<p>배깅의 대표적인 알고리즘 : RandomForest</p>\n</li>\n</ul>\n<h4 id=\"부트스트랩-샘플링-방식\" style=\"position:relative;\"><a href=\"#%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9-%EC%83%98%ED%94%8C%EB%A7%81-%EB%B0%A9%EC%8B%9D\" aria-label=\"부트스트랩 샘플링 방식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부트스트랩 샘플링 방식</h4>\n<blockquote>\n<p>부트스트랩은 통계학 분야에서 <strong>여러 작은 데이터 셋을 임의로 생성하여 개별 평균의 분포도를 측정하는 목적을 위한 샘플링 방식</strong></p>\n</blockquote>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 628px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.666666666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABaUlEQVQY0yXOzW/TMABA8f7/Zw4ctjMXhASiE6UbIEE7NI3RqFXWVrB0az6WLHHiJI4T2+VNdLd3+umNAKrOsIj1/8QdwFhLKDr2uWKft/TVLVauGKRPUgZEYkeX/mXQCq17+r5Ha41zjpE2jvmmZPvb55C8w0oPbeHrKuF8+ciXVURzf4oOXtMEp1xuP/J9fcbTz/c0eUZRlpRliRAFxhhGvYVP6zXV9Qkkb8FJ9HDA2wluAsHiLkf6M/p4jHqc4IdXLPdXqGxC1xZUsqauJVVVvYA4w2Y95j49pwiniA6MGVjsyiN4HUgqf8YQjzHpB27DH6z2c1Q6QbcFsm5omgYp5QtoRETvTfkVfWP2J2MbNxycY7pMufBC2ss39LtX1HcnnC1yJl7JZy8nnY9RIkNU8ogJIRiGgdE/IBMVcRIh8hRnBox1xGVHXCi67AEr15h6Q1LUREIRFQ366YG+a2mVQil1vLTW8gxBALvyxOAFggAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"rf_2.png\"\n        title=\"rf_2.png\"\n        src=\"/static/3fdadd610fcb37feac9c216653aefded/3d84d/rf_2.png\"\n        srcset=\"/static/3fdadd610fcb37feac9c216653aefded/e9ff0/rf_2.png 180w,\n/static/3fdadd610fcb37feac9c216653aefded/f21e7/rf_2.png 360w,\n/static/3fdadd610fcb37feac9c216653aefded/3d84d/rf_2.png 628w\"\n        sizes=\"(max-width: 628px) 100vw, 628px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</blockquote>\n<p>[출처] <a href=\"https://velog.io/@ayi4067/DAY27\">https://velog.io/@ayi4067/DAY27</a></p>\n<h4 id=\"진행과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\" aria-label=\"진행과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행과정</h4>\n<blockquote>\n<p><img src=\"/5176a9f1d8def6595977e42df1978823/rf_3.png\" alt=\"rf_3.PNG\"></p>\n</blockquote>\n<p>[출처] <a href=\"https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f\">https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f</a></p>\n<hr>\n<h4 id=\"하이퍼-파라미터-튜닝\" style=\"position:relative;\"><a href=\"#%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D\" aria-label=\"하이퍼 파라미터 튜닝 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>하이퍼 파라미터 튜닝</h4>\n<blockquote>\n<p>n_estimators : 랜덤 포레스트의 <strong>결정 트리의 갯수</strong>를 지정. <em>default값은 10</em></p>\n</blockquote>\n<blockquote>\n<p>max_features : 최적의 분할을 위해 고려해야 할 <strong>특징들의 갯수</strong></p>\n</blockquote>\n<blockquote>\n<p>min_sample_split : 노드 분할을 위한 <strong>최소 샘플 데이터의 갯수</strong></p>\n</blockquote>\n<blockquote>\n<p>min_samples_leaf : 말단 노드가 되기 위한 최소한의 샘플 수</p>\n</blockquote>\n<blockquote>\n<p>max_depth : 트리의 최대 깊이</p>\n</blockquote>\n<blockquote>\n<p>max_leaf_nodes : 말단 노드의 최대 갯수</p>\n</blockquote>\n<h3 id=\"oob-스코어\" style=\"position:relative;\"><a href=\"#oob-%EC%8A%A4%EC%BD%94%EC%96%B4\" aria-label=\"oob 스코어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>OOB 스코어</h3>\n<h4 id=\"oob-오차\" style=\"position:relative;\"><a href=\"#oob-%EC%98%A4%EC%B0%A8\" aria-label=\"oob 오차 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>OOB 오차</h4>\n<blockquote>\n<p>일반적으로 배깅 방식은 데이터셋의 2/3을 훈련 데이터로 사용한다. 나머지 1/3을 OOB(Out-of-Bag)관측치라 한다. <strong>OOB와 훈련 데이터셋의 예측과의 차이를 OOB 오차</strong>라고 하며 이 오차가 <strong>배깅 방식에서 평가 방식</strong>으로 사용된다. 이 OOB 오차를 통해 <em>OOB 스코어를 계산</em>한다.</p>\n</blockquote>\n<h3 id=\"변수-중요도\" style=\"position:relative;\"><a href=\"#%EB%B3%80%EC%88%98-%EC%A4%91%EC%9A%94%EB%8F%84\" aria-label=\"변수 중요도 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>변수 중요도</h3>\n<blockquote>\n<p>랜덤 포레스트의 장점 중 하나로 <strong>변수의 중요도를 구할 수 있다.</strong> 변수의 중요도를 파악하는 방법은 <em>어떤 변수가 분할 변수로 사용되고 사용된 후의 불순도(오차제곱합 or 지니계수)가 많이 감소되는 크기를 구한 후 평균</em>을 통해 구한다.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/0e431b3e2c2f960e5e965894d094298d/rf_4.png\" alt=\"rf_4.PNG\"></p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></p>\n<p>[2] <a href=\"https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8\">https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8</a></p>\n<p>[3] <a href=\"https://medium.com/nerd-for-tech/random-forest-sturdy-algorithm-d60b9f9140d4\">https://medium.com/nerd-for-tech/random-forest-sturdy-algorithm-d60b9f9140d4</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<ul>\n<li><a href=\"#keywords\">Keywords</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#randomforest\">RandomForest</a></p>\n<ul>\n<li>\n<p><a href=\"#%EB%B0%B0%EA%B9%85\">배깅</a></p>\n<ul>\n<li><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%98-%ED%8A%B9%EC%A7%95\">배깅의 특징</a></li>\n<li><a href=\"#%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9-%EC%83%98%ED%94%8C%EB%A7%81-%EB%B0%A9%EC%8B%9D\">부트스트랩 샘플링 방식</a></li>\n<li><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\">진행과정</a></li>\n<li><a href=\"#%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D\">하이퍼 파라미터 튜닝</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#oob-%EC%8A%A4%EC%BD%94%EC%96%B4\">OOB 스코어</a></p>\n<ul>\n<li><a href=\"#oob-%EC%98%A4%EC%B0%A8\">OOB 오차</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B3%80%EC%88%98-%EC%A4%91%EC%9A%94%EB%8F%84\">변수 중요도</a></p>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","excerpt":"Keywords RandomForest 배깅 하이퍼 파라미터 튜닝 OOB Score 변수 중요도 RandomForest 랜덤포레스트는 분류 및 회귀 ML 중 하나로 앙상블 학습 방법의 일종 으로 트리 기반 알고리즘이다. 각 트리들은 랜덤하게 서로 다른 특성을 가진다. 이를 통해 각 트리들의 예측이 비상관적이며 결과적으로 일반화 성능을 향상시킨다. 랜덤화는 각 트리들의 훈련 과정에서 진행되며, 랜덤 학습 데이터 추출 방법을 이용한 앙상블 학습법인 배깅(bagging)과 랜덤 노드 최적화(randomized node optimization)가 자주 사용된다. 이 두 가지 방법은 서로 동시에 사용되어 랜덤화 특성을 더욱 증진 시킬 수 있다.  [출처] https://velog.io/@ayi4067/DAY27 배깅 랜덤 샘플링한 데이터를 여러 모델에 학습시킨 뒤 결과를 집계하는 방식 배깅의 특징 배깅의 경우 각 분류기가 서로 독립적으로 병렬적으로 진행 범주형 자료일 때 다수결로 채택, 숫…","frontmatter":{"date":"September 18, 2022","title":"RandomForest","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] RandomForest/"}},"next":{"id":"849eb3fd-8385-55b6-af45-ca82ffb6acc6","html":"<h2 id=\"svm\" style=\"position:relative;\"><a href=\"#svm\" aria-label=\"svm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SVM</h2>\n<h3 id=\"소개\" style=\"position:relative;\"><a href=\"#%EC%86%8C%EA%B0%9C\" aria-label=\"소개 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>소개</h3>\n<p>SVM (Support Vector Machine)은 지도학습 모델 중 하나로 분류 및 회귀 분석에 사용되는 머신 러닝 기법</p>\n<p><img src=\"/65d97514013db4043faee8610b9194ef/svm_1.png\" alt=\"svm_1.PNG\"></p>\n<h3 id=\"주요-용어\" style=\"position:relative;\"><a href=\"#%EC%A3%BC%EC%9A%94-%EC%9A%A9%EC%96%B4\" aria-label=\"주요 용어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>주요 용어</h3>\n<p><img src=\"/0c98c7f17fb14118f1649eac1d3d5562/svm_2.png\" alt=\"svm_2.PNG\"></p>\n<h3 id=\"경계선을-정하는-기준\" style=\"position:relative;\"><a href=\"#%EA%B2%BD%EA%B3%84%EC%84%A0%EC%9D%84-%EC%A0%95%ED%95%98%EB%8A%94-%EA%B8%B0%EC%A4%80\" aria-label=\"경계선을 정하는 기준 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>경계선을 정하는 기준</h3>\n<p>아래 그림처럼 다양한 경계선이 생성될 수 있다. 어떠한 경계선을 선택하는 것이 옳을까?\n<img src=\"/743f9331cc388008dedb5183b3f13bf9/svm_3.png\" alt=\"svm_3.PNG\"></p>\n<p>margin이 크다는 의미는 경계선의 폭이 크다는 의미로 새로운 데이터 값이 들어오는 경우에 margin의 클 수록 더 정확한 분류가 가능</p>\n<h3 id=\"커널-트릭\" style=\"position:relative;\"><a href=\"#%EC%BB%A4%EB%84%90-%ED%8A%B8%EB%A6%AD\" aria-label=\"커널 트릭 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>커널 트릭</h3>\n<p>커널 트릭이란 저차원 -> 고차원 공간으로 매핑하는 작업을 말한다. 아래 그림과 같이 경계선을 생성 불가능 할때 주로 사용.\n<img src=\"/d4392ce275cc5360e861fcfb920389b9/svm_4.png\" alt=\"svm_4.PNG\"></p>\n<h3 id=\"주요-파라미터\" style=\"position:relative;\"><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0\" aria-label=\"주요 파라미터 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>주요 파라미터</h3>\n<ul>\n<li>C</li>\n</ul>\n<blockquote>\n<p>C는 오분류된 데이터의 허용 정도를 의미하며 이 값이 작을수록 많이 허용하고 높을 수록 적게 허용한다. 이로 인해 C값이 작으면 경계선 모양이 직선에 가까우며 높을 수록 여러겹의 굽은 선에 가깝다.</p>\n</blockquote>\n<ul>\n<li>Gamma</li>\n</ul>\n<blockquote>\n<p>각 데이터가 영향을 주는 정도를 나타내며 Gamma 값이 낮아지면 각 데이터의 영향력이 커져 큰 범위를 형성하는 반면 Gamma 값이 높아지면 각 데이터의 영향력이 적어져 좁은 범위를 형성한다.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/d672f117b8c19f50c199f1fba85721c4/svm_5.png\" alt=\"svm_5.PNG\"></p>\n</blockquote>\n<p>출처 : <a href=\"https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\">https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html</a></p>\n<ul>\n<li>Kernel 함수</li>\n</ul>\n<blockquote>\n<p>커널 함수는 데이터를 필요한 형식으로 변환하는 함수로 SVM에서는 선형, 비선형, RBF, 시그모이드 등의 여러 커널 함수가 사용된다. 주로 사용되는 커널 함수는 RBF(가우스 방사형 기저함수)로 데이터에 대한 사전 지식없이 사용 가능하며 앞서 언급한 Kernel 트릭에서 커널 함수로 매핑한 데이터를 분류하는데 사용.</p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://www.youtube.com/watch?v=efR1C6CvhmE\">https://www.youtube.com/watch?v=efR1C6CvhmE</a></p>\n<p>[2] <a href=\"https://scikit-learn.org/stable/modules/svm.html\">https://scikit-learn.org/stable/modules/svm.html</a></p>\n<p>[3] <a href=\"https://bskyvision.com/entry/%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0SVM%EC%9D%98-%EC%82%AC%EC%9A%A9%EC%9E%90%EB%A1%9C%EC%84%9C-%EA%BC%AD-%EC%95%8C%EC%95%84%EC%95%BC%ED%95%A0-%EA%B2%83%EB%93%A4-%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98-C%EC%99%80-gamma\">https://bskyvision.com/entry/%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0SVM%EC%9D%98-%EC%82%AC%EC%9A%A9%EC%9E%90%EB%A1%9C%EC%84%9C-%EA%BC%AD-%EC%95%8C%EC%95%84%EC%95%BC%ED%95%A0-%EA%B2%83%EB%93%A4-%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98-C%EC%99%80-gamma</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#svm\">SVM</a></p>\n<ul>\n<li><a href=\"#%EC%86%8C%EA%B0%9C\">소개</a></li>\n<li><a href=\"#%EC%A3%BC%EC%9A%94-%EC%9A%A9%EC%96%B4\">주요 용어</a></li>\n<li><a href=\"#%EA%B2%BD%EA%B3%84%EC%84%A0%EC%9D%84-%EC%A0%95%ED%95%98%EB%8A%94-%EA%B8%B0%EC%A4%80\">경계선을 정하는 기준</a></li>\n<li><a href=\"#%EC%BB%A4%EB%84%90-%ED%8A%B8%EB%A6%AD\">커널 트릭</a></li>\n<li><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0\">주요 파라미터</a></li>\n<li><a href=\"#reference\">Reference</a></li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"September 17, 2022","title":"SVM","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] SVM/"}},"prev":{"id":"faafee8b-fcf8-5f06-b2ac-3cbf460b6e14","html":"<h2 id=\"boosting-algorithm\" style=\"position:relative;\"><a href=\"#boosting-algorithm\" aria-label=\"boosting algorithm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Boosting Algorithm</h2>\n<h3 id=\"부스팅\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\" aria-label=\"부스팅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스팅</h3>\n<p>부스팅은 앙상블 학습 유형 중 하나로 <strong>약한 분류기를 순차적으로 학습-예측하면서 가중치를 조정하여 오류를 개선</strong>하면서 학습하는 방식.</p>\n<h4 id=\"대표적인-부스팅-머신러닝\" style=\"position:relative;\"><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%B6%80%EC%8A%A4%ED%8C%85-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D\" aria-label=\"대표적인 부스팅 머신러닝 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>대표적인 부스팅 머신러닝</h4>\n<blockquote>\n<p>AdaBoost</p>\n</blockquote>\n<blockquote>\n<p>GradientBoost</p>\n</blockquote>\n<blockquote>\n<p>XGBoost, LightGBM, CatBoost 등</p>\n</blockquote>\n<h3 id=\"adaboost\" style=\"position:relative;\"><a href=\"#adaboost\" aria-label=\"adaboost permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AdaBoost</h3>\n<blockquote>\n<p><strong>예측 성능이 낮은 학습기를 구축 및 조합하여 가중치 조절을 통해 좋은 성능을 발휘하는 강한 분류기를 합성하는 알고리즘</strong></p>\n</blockquote>\n<h4 id=\"진행-과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\" aria-label=\"진행 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행 과정</h4>\n<blockquote>\n<p><img src=\"/8aeb4d0b52fe1469868424cb8102fa68/bs_1.png\" alt=\"bs_1.PNG\"></p>\n</blockquote>\n<h4 id=\"정리\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EB%A6%AC\" aria-label=\"정리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정리</h4>\n<p>AdaBoost는 매 단계마다 이전 분류기에서 오차가 크거나 오분류된 데이터들의 가중치를 크게하고 정분류된 데이터들의 가중치는 적게 설정한뒤 다음 단계의 학습데이터셋의 추출과정에 가중치에 비례하게 복원추출하여 새로운 데이터셋을 만들고 모형을 적합하는 과정을 거친다.</p>\n<p>이러한 반복 단계를 통해 가중치가 반영된 총 오류를 최소화하는 분류기를 선택하고, 선택된 분류기에서 얻은 가중치 및 오류를 얻고, 이를 가속화된 분류기를 개선하는 데 이용한다.</p>\n<hr>\n<h3 id=\"gradient-boost\" style=\"position:relative;\"><a href=\"#gradient-boost\" aria-label=\"gradient boost permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Gradient Boost</h3>\n<blockquote>\n<p>AdaBoost와 마찬가지로 예측력이 낮은 분류기를 토대로 반복 학습하여 강한 분류기를 생성하는 것. AdaBoost와 달리 반복과정을 통해 <strong>손실함수의 최소값을 찾는 과정으로 진행</strong>한다.</p>\n</blockquote>\n<blockquote>\n<p>Gradient Boost는 XGBoost, LightBoost 등의 토대가 되는 알고리즘</p>\n</blockquote>\n<blockquote>\n<p><strong>알아야할 용어</strong></p>\n</blockquote>\n<blockquote>\n<blockquote>\n<p><img src=\"/ca3da52336dbdb7ea578339a3676384e/bs_2.png\" alt=\"bs_2.PNG\"></p>\n</blockquote>\n</blockquote>\n<h4 id=\"진행과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\" aria-label=\"진행과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행과정</h4>\n<blockquote>\n<p><img src=\"/623dcce697912e872a7778026408b262/bs_3.png\" alt=\"bs_3.PNG\"></p>\n</blockquote>\n<blockquote>\n<p>좀 더 구체적으로 이해를 돕자면 손실함수가 RSS일때</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/afedd2baf462e0db5a992d2fe2bb28aa/bs_4.png\" alt=\"bs_4.PNG\"></p>\n</blockquote>\n<p>[출처] <a href=\"https://m.blog.naver.com/luvwithcat/222103025023\">https://m.blog.naver.com/luvwithcat/222103025023</a></p>\n<blockquote>\n<p>위와 같이 잔차를 획득하고 잔차에 대한 예측을 시행한 후 나온 결정트리의 (맨 아래 leaf의 값 중 하나 * 학습률)을 잔차와 더해 잔차의 값을 최소화하는 방향으로 진행.</p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-14-AdaBoost\">https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-14-AdaBoost</a></p>\n<p>[2] <a href=\"https://zephyrus1111.tistory.com/195\">https://zephyrus1111.tistory.com/195</a></p>\n<p>[3] <a href=\"https://www.youtube.com/watch?v=3CC4N4z3GJc\">https://www.youtube.com/watch?v=3CC4N4z3GJc</a></p>\n<p>[4] <a href=\"https://zephyrus1111.tistory.com/232?category=858748\">https://zephyrus1111.tistory.com/232?category=858748</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#boosting-algorithm\">Boosting Algorithm</a></p>\n<ul>\n<li>\n<p><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\">부스팅</a></p>\n<ul>\n<li><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%B6%80%EC%8A%A4%ED%8C%85-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D\">대표적인 부스팅 머신러닝</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#adaboost\">AdaBoost</a></p>\n<ul>\n<li><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\">진행 과정</a></li>\n<li><a href=\"#%EC%A0%95%EB%A6%AC\">정리</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#gradient-boost\">Gradient Boost</a></p>\n<ul>\n<li><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\">진행과정</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"September 25, 2022","title":"Boosting Alogrithms","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] Boosting_Algorithm/"}},"site":{"siteMetadata":{"siteUrl":"https://han-archives.github.io","comments":{"utterances":{"repo":"Han-Archives/han-archives.github.io"}}}}},"pageContext":{"slug":"/ML/[ML] RandomForest/","nextSlug":"/ML/[ML] SVM/","prevSlug":"/ML/[ML] Boosting_Algorithm/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}