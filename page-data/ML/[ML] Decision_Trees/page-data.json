{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ML/[ML] Decision_Trees/",
    "result": {"data":{"cur":{"id":"b2ade22d-c2e0-5e32-b0ce-fb2a910908f8","html":"<h2 id=\"decision-trees\" style=\"position:relative;\"><a href=\"#decision-trees\" aria-label=\"decision trees permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Trees</h2>\n<p>Decision Tree(결정 트리)는 <em>지도 학습에서 분류 및 회귀에 사용되는 모델</em> 중 하나로 불순도가 낮아지는 방향으로 가지를 계속해서 분할.</p>\n<p><strong>ML 알고리즘 중에서 가장 직관적인 알고리즘</strong></p>\n<p><img src=\"/7c1dd10e6792b06c83464951abadf378/dt_main.png\" alt=\"dt_main.PNG\"></p>\n<h3 id=\"특징\" style=\"position:relative;\"><a href=\"#%ED%8A%B9%EC%A7%95\" aria-label=\"특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>특징</h3>\n<ul>\n<li>쉽게 이해할 수 있고 해석이 간편하다.</li>\n<li>별도의 전처리 없이 쉽게 사용이 가능하다.</li>\n<li>RandomForest 모형의 구성 요소</li>\n</ul>\n<h3 id=\"불순도\" style=\"position:relative;\"><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84\" aria-label=\"불순도 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>불순도</h3>\n<p><em>불순도란 다양한 요소들이 섞여있는 정도</em>를 의미. 대표적인 불순도 척도로 <strong>지니 계수</strong>와 <strong>엔트로피</strong>가 사용된다.</p>\n<h4 id=\"지니-계수\" style=\"position:relative;\"><a href=\"#%EC%A7%80%EB%8B%88-%EA%B3%84%EC%88%98\" aria-label=\"지니 계수 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>지니 계수</h4>\n<p>지니 계수란 경제적 불평등을 나타내는 용어로 <strong>0에 가까울 수록 평등하고 1에 가까울 수록 불평등</strong>을 나타낸다.</p>\n<p>의사 결정 트리에서의 지니계수는 이와 약간 달리 <strong>0.5값을 가질 때를 가장 불순도가 높다</strong>고 판단하며 지니계수가 0에 근접하도록 분할을 진행한다.</p>\n<p><strong>&#x3C;지니계수를 구하는 공식></strong>\n<img src=\"/f150531f7741a01be87673fa5b85de6a/dt_gini.png\" alt=\"dt_gini.PNG\"></p>\n<h4 id=\"엔트로피\" style=\"position:relative;\"><a href=\"#%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC\" aria-label=\"엔트로피 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>엔트로피</h4>\n<p>엔트로피란 정보이득을 나타내는 지표로 순도가 높을 때 얻는 정보 이득은 증가, 불순도가 높을수록 얻는 정보 이득은 감소. 지니 계수와 마찬가지로 엔트로피가 0에 가까울수록 좋으며 불순도가 낮아진다는 의미로 해석할 수 있다.</p>\n<p><strong>&#x3C;엔트로피를 구하는 공식></strong>\n<img src=\"/551fc7b929257a72f77b6b25ee5a07d2/dt_entropy.png\" alt=\"dt_entropy.PNG\"></p>\n<h4 id=\"불순도를-줄여나가는-과정\" style=\"position:relative;\"><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84%EB%A5%BC-%EC%A4%84%EC%97%AC%EB%82%98%EA%B0%80%EB%8A%94-%EA%B3%BC%EC%A0%95\" aria-label=\"불순도를 줄여나가는 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>불순도를 줄여나가는 과정</h4>\n<p><img src=\"/d160bb42a5bf9078537b95d8204c938a/dt_0.png\" alt=\"dt_0.PNG\"></p>\n<h3 id=\"decision-tree-알고리즘-유형\" style=\"position:relative;\"><a href=\"#decision-tree-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9C%A0%ED%98%95\" aria-label=\"decision tree 알고리즘 유형 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Tree 알고리즘 유형</h3>\n<ul>\n<li>ID3</li>\n</ul>\n<blockquote>\n<p>반복적으로 이분하는 알고리즘. 불순도로 엔트로피를 사용하며 독립변수가 범주형일 때만 사용가능. 연속형 독립변수 사용 불가.</p>\n</blockquote>\n<ul>\n<li>C4.5</li>\n</ul>\n<blockquote>\n<p>ID3의 단점을 보완한 알고리즘. 연속형 독립변수도 사용 가능</p>\n</blockquote>\n<ul>\n<li>CART (Classification And Regression Tree)</li>\n</ul>\n<blockquote>\n<p>가장 널리 사용되는 알고리즘. 분류 및 회귀 문제에서 사용 가능하며 분류 문제의 경우 불순도로 지니계수를, 회귀 문제의 경우 불순도로 분산을 사용하여 분류 진행.</p>\n</blockquote>\n<ul>\n<li>Chi-square (Chi-square automatic interaction detection)</li>\n</ul>\n<blockquote>\n<p>CART 알고리즘과 마찬가지로 분류 및 회귀 문제에서 사용 가능. 분류 문제의 경우 불순도로 카이제곱 검정값을, 회귀 문제의 경우 F 검정값으로 다지 분할을 진행</p>\n</blockquote>\n<h4 id=\"decision-tree-parameter\" style=\"position:relative;\"><a href=\"#decision-tree-parameter\" aria-label=\"decision tree parameter permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Tree Parameter</h4>\n<p>결정 트리 알고리즘에서 설정해야 할 파라미터는 다음과 같다.</p>\n<ul>\n<li><strong>min_sample_split</strong></li>\n</ul>\n<blockquote>\n<p>노드 분할을 위한 최소 샘플 데이터의 수</p>\n</blockquote>\n<ul>\n<li><strong>min_samples_leaf</strong></li>\n</ul>\n<blockquote>\n<p>말단 노드가 되기 위한 최소한의 샘플 수</p>\n</blockquote>\n<ul>\n<li><strong>max_features</strong></li>\n</ul>\n<blockquote>\n<p>최적의 분할을 위해 고려해야 할 특징들의 갯수</p>\n</blockquote>\n<ul>\n<li><strong>max_depth</strong></li>\n</ul>\n<blockquote>\n<p>트리의 최대 깊이</p>\n</blockquote>\n<ul>\n<li><strong>max_leaf_nodes</strong></li>\n</ul>\n<blockquote>\n<p>말단 노드의 최대 개수</p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://di-bigdata-study.tistory.com/2\">https://di-bigdata-study.tistory.com/2</a></p>\n<p>[2] <a href=\"https://scikit-learn.org/stable/modules/tree.html\">https://scikit-learn.org/stable/modules/tree.html</a></p>\n<p>[3] <a href=\"https://www.jcchouinard.com/decision-trees-in-machine-learning/\">https://www.jcchouinard.com/decision-trees-in-machine-learning/</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#decision-trees\">Decision Trees</a></p>\n<ul>\n<li>\n<p><a href=\"#%ED%8A%B9%EC%A7%95\">특징</a></p>\n</li>\n<li>\n<p><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84\">불순도</a></p>\n<ul>\n<li><a href=\"#%EC%A7%80%EB%8B%88-%EA%B3%84%EC%88%98\">지니 계수</a></li>\n<li><a href=\"#%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC\">엔트로피</a></li>\n<li><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84%EB%A5%BC-%EC%A4%84%EC%97%AC%EB%82%98%EA%B0%80%EB%8A%94-%EA%B3%BC%EC%A0%95\">불순도를 줄여나가는 과정</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#decision-tree-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9C%A0%ED%98%95\">Decision Tree 알고리즘 유형</a></p>\n<ul>\n<li><a href=\"#decision-tree-parameter\">Decision Tree Parameter</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","excerpt":"Decision Trees Decision Tree(결정 트리)는 지도 학습에서 분류 및 회귀에 사용되는 모델 중 하나로 불순도가 낮아지는 방향으로 가지를 계속해서 분할. ML 알고리즘 중에서 가장 직관적인 알고리즘 dt_main.PNG 특징 쉽게 이해할 수 있고 해석이 간편하다. 별도의 전처리 없이 쉽게 사용이 가능하다. RandomForest 모형의 구성 요소 불순도 불순도란 다양한 요소들이 섞여있는 정도를 의미. 대표적인 불순도 척도로 지니 계수와 엔트로피가 사용된다. 지니 계수 지니 계수란 경제적 불평등을 나타내는 용어로 0에 가까울 수록 평등하고 1에 가까울 수록 불평등을 나타낸다. 의사 결정 트리에서의 지니계수는 이와 약간 달리 0.5값을 가질 때를 가장 불순도가 높다고 판단하며 지니계수가 0에 근접하도록 분할을 진행한다. <지니계수를 구하는 공식>\ndt_gini.PNG 엔트로피 엔트로피란 정보이득을 나타내는 지표로 순도가 높을 때 얻는 정보 이득은 증가, 불순도가 높을…","frontmatter":{"date":"September 25, 2022","title":"Decision_Trees","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] Decision_Trees/"}},"next":{"id":"b22a1546-b405-5a09-af07-080c0d044953","html":"<h3 id=\"keywords\" style=\"position:relative;\"><a href=\"#keywords\" aria-label=\"keywords permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Keywords</h3>\n<ul>\n<li>RandomForest</li>\n<li>배깅</li>\n<li>하이퍼 파라미터 튜닝</li>\n<li>OOB Score</li>\n<li>변수 중요도</li>\n</ul>\n<h2 id=\"randomforest\" style=\"position:relative;\"><a href=\"#randomforest\" aria-label=\"randomforest permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RandomForest</h2>\n<blockquote>\n<p>랜덤포레스트는 <strong>분류 및 회귀 ML 중 하나로 앙상블 학습 방법의 일종</strong> 으로 트리 기반 알고리즘이다. <strong>각 트리들은 랜덤하게 서로 다른 특성</strong>을 가진다. 이를 통해 각 트리들의 예측이 <strong>비상관적</strong>이며 <strong>결과적으로 일반화 성능을 향상</strong>시킨다.</p>\n</blockquote>\n<blockquote>\n<p>랜덤화는 각 트리들의 훈련 과정에서 진행되며, <strong>랜덤 학습 데이터 추출 방법을 이용한 앙상블 학습법인 배깅(bagging)과 랜덤 노드 최적화(randomized node optimization)가 자주 사용된다</strong>. 이 두 가지 방법은 서로 동시에 사용되어 랜덤화 특성을 더욱 증진 시킬 수 있다.</p>\n</blockquote>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.22222222222223%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAACTklEQVQ4y32TTU8TURSGu3TjxkT/AjtNICaEjSw0Gv0D7HDhwn8gcWFMjB/QosQoSMAEKEOkK0sLloS2JogYbRqjoIBYUCraDobSmc50OvfOY6YffFk9ybl5c/Ke95x7zr0eAMdxDriUjhtm8YfOjecpboXW8Ea+oRcF9fi1mGue3eA+F1XB+a852voXaH/6iav+JXKGvZdc9cPCnsPV9ne4Y9h8TGt82NBY/lXAFpJ6/AOC7uHmC7cgUBJgCbBlJb7f7CqvaDu7OS6vZq6oJ5lM4h9VmAyHicViPBsfZyIUIh6PEQgECE5MMDMzw9CIn2AwSDQaZWh4mKmpKaanI7tYiMp8PS759t0unvQPoCgKXl8PvX39KGNjdPnu87i3j/FAAF93DwODg2Xc6e1meMTPqKLQ6X1QxrZtV6/str01hzR+UsNOYbOCs3EcU4VSDqlGQVbmIzMRKBVACKQaA9vau7IuYVtfJ1fUyAnI6ets13Bhk23L4rdVIpNPkynBVgky+e+olkB1cW6FrCWp7MTB07FocCRkcPplgfNzeU5EDE7FdS681jj+osDJ6A4X5zSOTeo0xvNcmtc4OmnQ+kqnddbFJmdmNUxR3XIgbdH2VqNjwaD7S5H2hM41F6+YtCcNrq86PErDlQWbm59NHq5WOPeWTXwrJpcTGneWTMTes3Go70DRQK6lsFIp2NwAR9QeSF1+eYausDzkoroxNRzmTUMD75qaSDQ3U1TVcqqQ8q+c//+UmmAwSKKxkfdnz5FsacHKZiudSPnP//wHJwKqnS7X0FsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"rf_1.png\"\n        title=\"rf_1.png\"\n        src=\"/static/e671b4b0021826413ff8f13be067e3fb/37523/rf_1.png\"\n        srcset=\"/static/e671b4b0021826413ff8f13be067e3fb/e9ff0/rf_1.png 180w,\n/static/e671b4b0021826413ff8f13be067e3fb/f21e7/rf_1.png 360w,\n/static/e671b4b0021826413ff8f13be067e3fb/37523/rf_1.png 720w,\n/static/e671b4b0021826413ff8f13be067e3fb/c483d/rf_1.png 751w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</blockquote>\n<p>[출처] <a href=\"https://velog.io/@ayi4067/DAY27\">https://velog.io/@ayi4067/DAY27</a></p>\n<hr>\n<h3 id=\"배깅\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85\" aria-label=\"배깅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅</h3>\n<blockquote>\n<p>랜덤 샘플링한 데이터를 여러 모델에 학습시킨 뒤 결과를 집계하는 방식</p>\n</blockquote>\n<h4 id=\"배깅의-특징\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%98-%ED%8A%B9%EC%A7%95\" aria-label=\"배깅의 특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅의 특징</h4>\n<ul>\n<li>\n<p>배깅의 경우 각 분류기가 서로 독립적으로 병렬적으로 진행</p>\n</li>\n<li>\n<p>범주형 자료일 때 다수결로 채택, 숫자형 자료일 때 평균 값을 채택</p>\n</li>\n<li>\n<p>속도가 빠르며 과적합 영향이 적다.</p>\n</li>\n<li>\n<p>적은 데이터셋이라도 준수한 결과를 도출한다.</p>\n</li>\n<li>\n<p>배깅의 대표적인 알고리즘 : RandomForest</p>\n</li>\n</ul>\n<h4 id=\"부트스트랩-샘플링-방식\" style=\"position:relative;\"><a href=\"#%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9-%EC%83%98%ED%94%8C%EB%A7%81-%EB%B0%A9%EC%8B%9D\" aria-label=\"부트스트랩 샘플링 방식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부트스트랩 샘플링 방식</h4>\n<blockquote>\n<p>부트스트랩은 통계학 분야에서 <strong>여러 작은 데이터 셋을 임의로 생성하여 개별 평균의 분포도를 측정하는 목적을 위한 샘플링 방식</strong></p>\n</blockquote>\n<blockquote>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 628px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.666666666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABaUlEQVQY0yXOzW/TMABA8f7/Zw4ctjMXhASiE6UbIEE7NI3RqFXWVrB0az6WLHHiJI4T2+VNdLd3+umNAKrOsIj1/8QdwFhLKDr2uWKft/TVLVauGKRPUgZEYkeX/mXQCq17+r5Ha41zjpE2jvmmZPvb55C8w0oPbeHrKuF8+ciXVURzf4oOXtMEp1xuP/J9fcbTz/c0eUZRlpRliRAFxhhGvYVP6zXV9Qkkb8FJ9HDA2wluAsHiLkf6M/p4jHqc4IdXLPdXqGxC1xZUsqauJVVVvYA4w2Y95j49pwiniA6MGVjsyiN4HUgqf8YQjzHpB27DH6z2c1Q6QbcFsm5omgYp5QtoRETvTfkVfWP2J2MbNxycY7pMufBC2ss39LtX1HcnnC1yJl7JZy8nnY9RIkNU8ogJIRiGgdE/IBMVcRIh8hRnBox1xGVHXCi67AEr15h6Q1LUREIRFQ366YG+a2mVQil1vLTW8gxBALvyxOAFggAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"rf_2.png\"\n        title=\"rf_2.png\"\n        src=\"/static/3fdadd610fcb37feac9c216653aefded/3d84d/rf_2.png\"\n        srcset=\"/static/3fdadd610fcb37feac9c216653aefded/e9ff0/rf_2.png 180w,\n/static/3fdadd610fcb37feac9c216653aefded/f21e7/rf_2.png 360w,\n/static/3fdadd610fcb37feac9c216653aefded/3d84d/rf_2.png 628w\"\n        sizes=\"(max-width: 628px) 100vw, 628px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n</blockquote>\n<p>[출처] <a href=\"https://velog.io/@ayi4067/DAY27\">https://velog.io/@ayi4067/DAY27</a></p>\n<h4 id=\"진행과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\" aria-label=\"진행과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행과정</h4>\n<blockquote>\n<p><img src=\"/5176a9f1d8def6595977e42df1978823/rf_3.png\" alt=\"rf_3.PNG\"></p>\n</blockquote>\n<p>[출처] <a href=\"https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f\">https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f</a></p>\n<hr>\n<h4 id=\"하이퍼-파라미터-튜닝\" style=\"position:relative;\"><a href=\"#%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D\" aria-label=\"하이퍼 파라미터 튜닝 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>하이퍼 파라미터 튜닝</h4>\n<blockquote>\n<p>n_estimators : 랜덤 포레스트의 <strong>결정 트리의 갯수</strong>를 지정. <em>default값은 10</em></p>\n</blockquote>\n<blockquote>\n<p>max_features : 최적의 분할을 위해 고려해야 할 <strong>특징들의 갯수</strong></p>\n</blockquote>\n<blockquote>\n<p>min_sample_split : 노드 분할을 위한 <strong>최소 샘플 데이터의 갯수</strong></p>\n</blockquote>\n<blockquote>\n<p>min_samples_leaf : 말단 노드가 되기 위한 최소한의 샘플 수</p>\n</blockquote>\n<blockquote>\n<p>max_depth : 트리의 최대 깊이</p>\n</blockquote>\n<blockquote>\n<p>max_leaf_nodes : 말단 노드의 최대 갯수</p>\n</blockquote>\n<h3 id=\"oob-스코어\" style=\"position:relative;\"><a href=\"#oob-%EC%8A%A4%EC%BD%94%EC%96%B4\" aria-label=\"oob 스코어 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>OOB 스코어</h3>\n<h4 id=\"oob-오차\" style=\"position:relative;\"><a href=\"#oob-%EC%98%A4%EC%B0%A8\" aria-label=\"oob 오차 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>OOB 오차</h4>\n<blockquote>\n<p>일반적으로 배깅 방식은 데이터셋의 2/3을 훈련 데이터로 사용한다. 나머지 1/3을 OOB(Out-of-Bag)관측치라 한다. <strong>OOB와 훈련 데이터셋의 예측과의 차이를 OOB 오차</strong>라고 하며 이 오차가 <strong>배깅 방식에서 평가 방식</strong>으로 사용된다. 이 OOB 오차를 통해 <em>OOB 스코어를 계산</em>한다.</p>\n</blockquote>\n<h3 id=\"변수-중요도\" style=\"position:relative;\"><a href=\"#%EB%B3%80%EC%88%98-%EC%A4%91%EC%9A%94%EB%8F%84\" aria-label=\"변수 중요도 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>변수 중요도</h3>\n<blockquote>\n<p>랜덤 포레스트의 장점 중 하나로 <strong>변수의 중요도를 구할 수 있다.</strong> 변수의 중요도를 파악하는 방법은 <em>어떤 변수가 분할 변수로 사용되고 사용된 후의 불순도(오차제곱합 or 지니계수)가 많이 감소되는 크기를 구한 후 평균</em>을 통해 구한다.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/0e431b3e2c2f960e5e965894d094298d/rf_4.png\" alt=\"rf_4.PNG\"></p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></p>\n<p>[2] <a href=\"https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8\">https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8</a></p>\n<p>[3] <a href=\"https://medium.com/nerd-for-tech/random-forest-sturdy-algorithm-d60b9f9140d4\">https://medium.com/nerd-for-tech/random-forest-sturdy-algorithm-d60b9f9140d4</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<ul>\n<li><a href=\"#keywords\">Keywords</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#randomforest\">RandomForest</a></p>\n<ul>\n<li>\n<p><a href=\"#%EB%B0%B0%EA%B9%85\">배깅</a></p>\n<ul>\n<li><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%98-%ED%8A%B9%EC%A7%95\">배깅의 특징</a></li>\n<li><a href=\"#%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9-%EC%83%98%ED%94%8C%EB%A7%81-%EB%B0%A9%EC%8B%9D\">부트스트랩 샘플링 방식</a></li>\n<li><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\">진행과정</a></li>\n<li><a href=\"#%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D\">하이퍼 파라미터 튜닝</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#oob-%EC%8A%A4%EC%BD%94%EC%96%B4\">OOB 스코어</a></p>\n<ul>\n<li><a href=\"#oob-%EC%98%A4%EC%B0%A8\">OOB 오차</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B3%80%EC%88%98-%EC%A4%91%EC%9A%94%EB%8F%84\">변수 중요도</a></p>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"September 18, 2022","title":"RandomForest","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] RandomForest/"}},"prev":{"id":"faafee8b-fcf8-5f06-b2ac-3cbf460b6e14","html":"<h2 id=\"boosting-algorithm\" style=\"position:relative;\"><a href=\"#boosting-algorithm\" aria-label=\"boosting algorithm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Boosting Algorithm</h2>\n<h3 id=\"부스팅\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\" aria-label=\"부스팅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스팅</h3>\n<p>부스팅은 앙상블 학습 유형 중 하나로 <strong>약한 분류기를 순차적으로 학습-예측하면서 가중치를 조정하여 오류를 개선</strong>하면서 학습하는 방식.</p>\n<h4 id=\"대표적인-부스팅-머신러닝\" style=\"position:relative;\"><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%B6%80%EC%8A%A4%ED%8C%85-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D\" aria-label=\"대표적인 부스팅 머신러닝 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>대표적인 부스팅 머신러닝</h4>\n<blockquote>\n<p>AdaBoost</p>\n</blockquote>\n<blockquote>\n<p>GradientBoost</p>\n</blockquote>\n<blockquote>\n<p>XGBoost, LightGBM, CatBoost 등</p>\n</blockquote>\n<h3 id=\"adaboost\" style=\"position:relative;\"><a href=\"#adaboost\" aria-label=\"adaboost permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AdaBoost</h3>\n<blockquote>\n<p><strong>예측 성능이 낮은 학습기를 구축 및 조합하여 가중치 조절을 통해 좋은 성능을 발휘하는 강한 분류기를 합성하는 알고리즘</strong></p>\n</blockquote>\n<h4 id=\"진행-과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\" aria-label=\"진행 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행 과정</h4>\n<blockquote>\n<p><img src=\"/8aeb4d0b52fe1469868424cb8102fa68/bs_1.png\" alt=\"bs_1.PNG\"></p>\n</blockquote>\n<h4 id=\"정리\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EB%A6%AC\" aria-label=\"정리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정리</h4>\n<p>AdaBoost는 매 단계마다 이전 분류기에서 오차가 크거나 오분류된 데이터들의 가중치를 크게하고 정분류된 데이터들의 가중치는 적게 설정한뒤 다음 단계의 학습데이터셋의 추출과정에 가중치에 비례하게 복원추출하여 새로운 데이터셋을 만들고 모형을 적합하는 과정을 거친다.</p>\n<p>이러한 반복 단계를 통해 가중치가 반영된 총 오류를 최소화하는 분류기를 선택하고, 선택된 분류기에서 얻은 가중치 및 오류를 얻고, 이를 가속화된 분류기를 개선하는 데 이용한다.</p>\n<hr>\n<h3 id=\"gradient-boost\" style=\"position:relative;\"><a href=\"#gradient-boost\" aria-label=\"gradient boost permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Gradient Boost</h3>\n<blockquote>\n<p>AdaBoost와 마찬가지로 예측력이 낮은 분류기를 토대로 반복 학습하여 강한 분류기를 생성하는 것. AdaBoost와 달리 반복과정을 통해 <strong>손실함수의 최소값을 찾는 과정으로 진행</strong>한다.</p>\n</blockquote>\n<blockquote>\n<p>Gradient Boost는 XGBoost, LightBoost 등의 토대가 되는 알고리즘</p>\n</blockquote>\n<blockquote>\n<p><strong>알아야할 용어</strong></p>\n</blockquote>\n<blockquote>\n<blockquote>\n<p><img src=\"/ca3da52336dbdb7ea578339a3676384e/bs_2.png\" alt=\"bs_2.PNG\"></p>\n</blockquote>\n</blockquote>\n<h4 id=\"진행과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\" aria-label=\"진행과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행과정</h4>\n<blockquote>\n<p><img src=\"/623dcce697912e872a7778026408b262/bs_3.png\" alt=\"bs_3.PNG\"></p>\n</blockquote>\n<blockquote>\n<p>좀 더 구체적으로 이해를 돕자면 손실함수가 RSS일때</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/afedd2baf462e0db5a992d2fe2bb28aa/bs_4.png\" alt=\"bs_4.PNG\"></p>\n</blockquote>\n<p>[출처] <a href=\"https://m.blog.naver.com/luvwithcat/222103025023\">https://m.blog.naver.com/luvwithcat/222103025023</a></p>\n<blockquote>\n<p>위와 같이 잔차를 획득하고 잔차에 대한 예측을 시행한 후 나온 결정트리의 (맨 아래 leaf의 값 중 하나 * 학습률)을 잔차와 더해 잔차의 값을 최소화하는 방향으로 진행.</p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-14-AdaBoost\">https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-14-AdaBoost</a></p>\n<p>[2] <a href=\"https://zephyrus1111.tistory.com/195\">https://zephyrus1111.tistory.com/195</a></p>\n<p>[3] <a href=\"https://www.youtube.com/watch?v=3CC4N4z3GJc\">https://www.youtube.com/watch?v=3CC4N4z3GJc</a></p>\n<p>[4] <a href=\"https://zephyrus1111.tistory.com/232?category=858748\">https://zephyrus1111.tistory.com/232?category=858748</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#boosting-algorithm\">Boosting Algorithm</a></p>\n<ul>\n<li>\n<p><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\">부스팅</a></p>\n<ul>\n<li><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%B6%80%EC%8A%A4%ED%8C%85-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D\">대표적인 부스팅 머신러닝</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#adaboost\">AdaBoost</a></p>\n<ul>\n<li><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\">진행 과정</a></li>\n<li><a href=\"#%EC%A0%95%EB%A6%AC\">정리</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#gradient-boost\">Gradient Boost</a></p>\n<ul>\n<li><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\">진행과정</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"September 25, 2022","title":"Boosting Alogrithms","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] Boosting_Algorithm/"}},"site":{"siteMetadata":{"siteUrl":"https://han-archives.github.io","comments":{"utterances":{"repo":"Han-Archives/han-archives.github.io"}}}}},"pageContext":{"slug":"/ML/[ML] Decision_Trees/","nextSlug":"/ML/[ML] RandomForest/","prevSlug":"/ML/[ML] Boosting_Algorithm/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}