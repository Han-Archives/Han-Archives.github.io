{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ML/[ML] Decision_Trees/",
    "result": {"data":{"cur":{"id":"b2ade22d-c2e0-5e32-b0ce-fb2a910908f8","html":"<h2 id=\"decision-trees\" style=\"position:relative;\"><a href=\"#decision-trees\" aria-label=\"decision trees permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Trees</h2>\n<p>Decision Tree(결정 트리)는 <em>지도 학습에서 분류 및 회귀에 사용되는 모델</em> 중 하나로 불순도가 낮아지는 방향으로 가지를 계속해서 분할.</p>\n<p><strong>ML 알고리즘 중에서 가장 직관적인 알고리즘</strong></p>\n<p><img src=\"/7c1dd10e6792b06c83464951abadf378/dt_main.png\" alt=\"dt_main.PNG\"></p>\n<h3 id=\"특징\" style=\"position:relative;\"><a href=\"#%ED%8A%B9%EC%A7%95\" aria-label=\"특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>특징</h3>\n<ul>\n<li>쉽게 이해할 수 있고 해석이 간편하다.</li>\n<li>별도의 전처리 없이 쉽게 사용이 가능하다.</li>\n<li>RandomForest 모형의 구성 요소</li>\n</ul>\n<h3 id=\"불순도\" style=\"position:relative;\"><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84\" aria-label=\"불순도 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>불순도</h3>\n<p><em>불순도란 다양한 요소들이 섞여있는 정도</em>를 의미. 대표적인 불순도 척도로 <strong>지니 계수</strong>와 <strong>엔트로피</strong>가 사용된다.</p>\n<h4 id=\"지니-계수\" style=\"position:relative;\"><a href=\"#%EC%A7%80%EB%8B%88-%EA%B3%84%EC%88%98\" aria-label=\"지니 계수 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>지니 계수</h4>\n<p>지니 계수란 경제적 불평등을 나타내는 용어로 <strong>0에 가까울 수록 평등하고 1에 가까울 수록 불평등</strong>을 나타낸다.</p>\n<p>의사 결정 트리에서의 지니계수는 이와 약간 달리 <strong>0.5값을 가질 때를 가장 불순도가 높다</strong>고 판단하며 지니계수가 0에 근접하도록 분할을 진행한다.</p>\n<p><strong>&#x3C;지니계수를 구하는 공식></strong>\n<img src=\"/f150531f7741a01be87673fa5b85de6a/dt_gini.png\" alt=\"dt_gini.PNG\"></p>\n<h4 id=\"엔트로피\" style=\"position:relative;\"><a href=\"#%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC\" aria-label=\"엔트로피 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>엔트로피</h4>\n<p>엔트로피란 정보이득을 나타내는 지표로 순도가 높을 때 얻는 정보 이득은 증가, 불순도가 높을수록 얻는 정보 이득은 감소. 지니 계수와 마찬가지로 엔트로피가 0에 가까울수록 좋으며 불순도가 낮아진다는 의미로 해석할 수 있다.</p>\n<p><strong>&#x3C;엔트로피를 구하는 공식></strong>\n<img src=\"/551fc7b929257a72f77b6b25ee5a07d2/dt_entropy.png\" alt=\"dt_entropy.PNG\"></p>\n<h4 id=\"불순도를-줄여나가는-과정\" style=\"position:relative;\"><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84%EB%A5%BC-%EC%A4%84%EC%97%AC%EB%82%98%EA%B0%80%EB%8A%94-%EA%B3%BC%EC%A0%95\" aria-label=\"불순도를 줄여나가는 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>불순도를 줄여나가는 과정</h4>\n<p><img src=\"/d160bb42a5bf9078537b95d8204c938a/dt_0.png\" alt=\"dt_0.PNG\"></p>\n<h3 id=\"decision-tree-알고리즘-유형\" style=\"position:relative;\"><a href=\"#decision-tree-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9C%A0%ED%98%95\" aria-label=\"decision tree 알고리즘 유형 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Tree 알고리즘 유형</h3>\n<ul>\n<li>ID3</li>\n</ul>\n<blockquote>\n<p>반복적으로 이분하는 알고리즘. 불순도로 엔트로피를 사용하며 독립변수가 범주형일 때만 사용가능. 연속형 독립변수 사용 불가.</p>\n</blockquote>\n<ul>\n<li>C4.5</li>\n</ul>\n<blockquote>\n<p>ID3의 단점을 보완한 알고리즘. 연속형 독립변수도 사용 가능</p>\n</blockquote>\n<ul>\n<li>CART (Classification And Regression Tree)</li>\n</ul>\n<blockquote>\n<p>가장 널리 사용되는 알고리즘. 분류 및 회귀 문제에서 사용 가능하며 분류 문제의 경우 불순도로 지니계수를, 회귀 문제의 경우 불순도로 분산을 사용하여 분류 진행.</p>\n</blockquote>\n<ul>\n<li>Chi-square (Chi-square automatic interaction detection)</li>\n</ul>\n<blockquote>\n<p>CART 알고리즘과 마찬가지로 분류 및 회귀 문제에서 사용 가능. 분류 문제의 경우 불순도로 카이제곱 검정값을, 회귀 문제의 경우 F 검정값으로 다지 분할을 진행</p>\n</blockquote>\n<h4 id=\"decision-tree-parameter\" style=\"position:relative;\"><a href=\"#decision-tree-parameter\" aria-label=\"decision tree parameter permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Tree Parameter</h4>\n<p>결정 트리 알고리즘에서 설정해야 할 파라미터는 다음과 같다.</p>\n<ul>\n<li><strong>min_sample_split</strong></li>\n</ul>\n<blockquote>\n<p>노드 분할을 위한 최소 샘플 데이터의 수</p>\n</blockquote>\n<ul>\n<li><strong>min_samples_leaf</strong></li>\n</ul>\n<blockquote>\n<p>말단 노드가 되기 위한 최소한의 샘플 수</p>\n</blockquote>\n<ul>\n<li><strong>max_features</strong></li>\n</ul>\n<blockquote>\n<p>최적의 분할을 위해 고려해야 할 특징들의 갯수</p>\n</blockquote>\n<ul>\n<li><strong>max_depth</strong></li>\n</ul>\n<blockquote>\n<p>트리의 최대 깊이</p>\n</blockquote>\n<ul>\n<li><strong>max_leaf_nodes</strong></li>\n</ul>\n<blockquote>\n<p>말단 노드의 최대 개수</p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://di-bigdata-study.tistory.com/2\">https://di-bigdata-study.tistory.com/2</a></p>\n<p>[2] <a href=\"https://scikit-learn.org/stable/modules/tree.html\">https://scikit-learn.org/stable/modules/tree.html</a></p>\n<p>[3] <a href=\"https://www.jcchouinard.com/decision-trees-in-machine-learning/\">https://www.jcchouinard.com/decision-trees-in-machine-learning/</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#decision-trees\">Decision Trees</a></p>\n<ul>\n<li>\n<p><a href=\"#%ED%8A%B9%EC%A7%95\">특징</a></p>\n</li>\n<li>\n<p><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84\">불순도</a></p>\n<ul>\n<li><a href=\"#%EC%A7%80%EB%8B%88-%EA%B3%84%EC%88%98\">지니 계수</a></li>\n<li><a href=\"#%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC\">엔트로피</a></li>\n<li><a href=\"#%EB%B6%88%EC%88%9C%EB%8F%84%EB%A5%BC-%EC%A4%84%EC%97%AC%EB%82%98%EA%B0%80%EB%8A%94-%EA%B3%BC%EC%A0%95\">불순도를 줄여나가는 과정</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#decision-tree-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9C%A0%ED%98%95\">Decision Tree 알고리즘 유형</a></p>\n<ul>\n<li><a href=\"#decision-tree-parameter\">Decision Tree Parameter</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","excerpt":"Decision Trees Decision Tree(결정 트리)는 지도 학습에서 분류 및 회귀에 사용되는 모델 중 하나로 불순도가 낮아지는 방향으로 가지를 계속해서 분할. ML 알고리즘 중에서 가장 직관적인 알고리즘 dt_main.PNG 특징 쉽게 이해할 수 있고 해석이 간편하다. 별도의 전처리 없이 쉽게 사용이 가능하다. RandomForest 모형의 구성 요소 불순도 불순도란 다양한 요소들이 섞여있는 정도를 의미. 대표적인 불순도 척도로 지니 계수와 엔트로피가 사용된다. 지니 계수 지니 계수란 경제적 불평등을 나타내는 용어로 0에 가까울 수록 평등하고 1에 가까울 수록 불평등을 나타낸다. 의사 결정 트리에서의 지니계수는 이와 약간 달리 0.5값을 가질 때를 가장 불순도가 높다고 판단하며 지니계수가 0에 근접하도록 분할을 진행한다. <지니계수를 구하는 공식>\ndt_gini.PNG 엔트로피 엔트로피란 정보이득을 나타내는 지표로 순도가 높을 때 얻는 정보 이득은 증가, 불순도가 높을…","frontmatter":{"date":"September 25, 2022","title":"Decision_Trees","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] Decision_Trees/"}},"next":{"id":"faafee8b-fcf8-5f06-b2ac-3cbf460b6e14","html":"<h2 id=\"boosting-algorithm\" style=\"position:relative;\"><a href=\"#boosting-algorithm\" aria-label=\"boosting algorithm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Boosting Algorithm</h2>\n<h3 id=\"부스팅\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\" aria-label=\"부스팅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스팅</h3>\n<p>부스팅은 앙상블 학습 유형 중 하나로 <strong>약한 분류기를 순차적으로 학습-예측하면서 가중치를 조정하여 오류를 개선</strong>하면서 학습하는 방식.</p>\n<h4 id=\"대표적인-부스팅-머신러닝\" style=\"position:relative;\"><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%B6%80%EC%8A%A4%ED%8C%85-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D\" aria-label=\"대표적인 부스팅 머신러닝 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>대표적인 부스팅 머신러닝</h4>\n<blockquote>\n<p>AdaBoost</p>\n</blockquote>\n<blockquote>\n<p>GradientBoost</p>\n</blockquote>\n<blockquote>\n<p>XGBoost, LightGBM, CatBoost 등</p>\n</blockquote>\n<h3 id=\"adaboost\" style=\"position:relative;\"><a href=\"#adaboost\" aria-label=\"adaboost permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AdaBoost</h3>\n<blockquote>\n<p><strong>예측 성능이 낮은 학습기를 구축 및 조합하여 가중치 조절을 통해 좋은 성능을 발휘하는 강한 분류기를 합성하는 알고리즘</strong></p>\n</blockquote>\n<h4 id=\"진행-과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\" aria-label=\"진행 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행 과정</h4>\n<blockquote>\n<p><img src=\"/8aeb4d0b52fe1469868424cb8102fa68/bs_1.png\" alt=\"bs_1.PNG\"></p>\n</blockquote>\n<h4 id=\"정리\" style=\"position:relative;\"><a href=\"#%EC%A0%95%EB%A6%AC\" aria-label=\"정리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>정리</h4>\n<p>AdaBoost는 매 단계마다 이전 분류기에서 오차가 크거나 오분류된 데이터들의 가중치를 크게하고 정분류된 데이터들의 가중치는 적게 설정한뒤 다음 단계의 학습데이터셋의 추출과정에 가중치에 비례하게 복원추출하여 새로운 데이터셋을 만들고 모형을 적합하는 과정을 거친다.</p>\n<p>이러한 반복 단계를 통해 가중치가 반영된 총 오류를 최소화하는 분류기를 선택하고, 선택된 분류기에서 얻은 가중치 및 오류를 얻고, 이를 가속화된 분류기를 개선하는 데 이용한다.</p>\n<hr>\n<h3 id=\"gradient-boost\" style=\"position:relative;\"><a href=\"#gradient-boost\" aria-label=\"gradient boost permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Gradient Boost</h3>\n<blockquote>\n<p>AdaBoost와 마찬가지로 예측력이 낮은 분류기를 토대로 반복 학습하여 강한 분류기를 생성하는 것. AdaBoost와 달리 반복과정을 통해 <strong>손실함수의 최소값을 찾는 과정으로 진행</strong>한다.</p>\n</blockquote>\n<blockquote>\n<p>Gradient Boost는 XGBoost, LightBoost 등의 토대가 되는 알고리즘</p>\n</blockquote>\n<blockquote>\n<p><strong>알아야할 용어</strong></p>\n</blockquote>\n<blockquote>\n<blockquote>\n<p><img src=\"/ca3da52336dbdb7ea578339a3676384e/bs_2.png\" alt=\"bs_2.PNG\"></p>\n</blockquote>\n</blockquote>\n<h4 id=\"진행과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\" aria-label=\"진행과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행과정</h4>\n<blockquote>\n<p><img src=\"/623dcce697912e872a7778026408b262/bs_3.png\" alt=\"bs_3.PNG\"></p>\n</blockquote>\n<blockquote>\n<p>좀 더 구체적으로 이해를 돕자면 손실함수가 RSS일때</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/afedd2baf462e0db5a992d2fe2bb28aa/bs_4.png\" alt=\"bs_4.PNG\"></p>\n</blockquote>\n<p>[출처] <a href=\"https://m.blog.naver.com/luvwithcat/222103025023\">https://m.blog.naver.com/luvwithcat/222103025023</a></p>\n<blockquote>\n<p>위와 같이 잔차를 획득하고 잔차에 대한 예측을 시행한 후 나온 결정트리의 (맨 아래 leaf의 값 중 하나 * 학습률)을 잔차와 더해 잔차의 값을 최소화하는 방향으로 진행.</p>\n</blockquote>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-14-AdaBoost\">https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-14-AdaBoost</a></p>\n<p>[2] <a href=\"https://zephyrus1111.tistory.com/195\">https://zephyrus1111.tistory.com/195</a></p>\n<p>[3] <a href=\"https://www.youtube.com/watch?v=3CC4N4z3GJc\">https://www.youtube.com/watch?v=3CC4N4z3GJc</a></p>\n<p>[4] <a href=\"https://zephyrus1111.tistory.com/232?category=858748\">https://zephyrus1111.tistory.com/232?category=858748</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#boosting-algorithm\">Boosting Algorithm</a></p>\n<ul>\n<li>\n<p><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\">부스팅</a></p>\n<ul>\n<li><a href=\"#%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%EB%B6%80%EC%8A%A4%ED%8C%85-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D\">대표적인 부스팅 머신러닝</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#adaboost\">AdaBoost</a></p>\n<ul>\n<li><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\">진행 과정</a></li>\n<li><a href=\"#%EC%A0%95%EB%A6%AC\">정리</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#gradient-boost\">Gradient Boost</a></p>\n<ul>\n<li><a href=\"#%EC%A7%84%ED%96%89%EA%B3%BC%EC%A0%95\">진행과정</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"September 25, 2022","title":"Boosting Alogrithms","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] Boosting_Algorithm/"}},"prev":{"id":"3fa11651-8acf-5d81-bcdc-ad895235be35","html":"<h2 id=\"ensemble\" style=\"position:relative;\"><a href=\"#ensemble\" aria-label=\"ensemble permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Ensemble</h2>\n<p>앙상블 기법이란 한마디로 쉽게 설명하자면 <strong>여러 전문가(ML)들이 협력하여 결론(예측)을 하는 방식</strong>.</p>\n<p><img src=\"/93ced71a7b96e5ecde96fd534997c98c/ens_1.png\" alt=\"ens_1.PNG\"></p>\n<h3 id=\"앙상블-학습-유형\" style=\"position:relative;\"><a href=\"#%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-%EC%9C%A0%ED%98%95\" aria-label=\"앙상블 학습 유형 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>앙상블 학습 유형</h3>\n<p>대표적인 3가지 학습 유형으로 <strong>배깅, 보팅, 부스팅 3가지 방법</strong>이 있다.</p>\n<h3 id=\"배깅bootstrap-aggregating-bagging\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85bootstrap-aggregating-bagging\" aria-label=\"배깅bootstrap aggregating bagging permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅(Bootstrap Aggregating, Bagging)</h3>\n<h4 id=\"배깅이란\" style=\"position:relative;\"><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%B4%EB%9E%80\" aria-label=\"배깅이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>배깅이란?</h4>\n<blockquote>\n<p>랜덤 샘플링한 데이터를 여러 모델에 학습시킨 뒤 결과를 집계하는 방식</p>\n</blockquote>\n<h4 id=\"특징\" style=\"position:relative;\"><a href=\"#%ED%8A%B9%EC%A7%95\" aria-label=\"특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>특징</h4>\n<ul>\n<li>\n<p>배깅의 경우 각 분류기가 서로 독립적으로 병렬적으로 진행</p>\n</li>\n<li>\n<p>범주형 자료일 때 다수결로 채택, 숫자형 자료일 때 평균 값을 채택</p>\n</li>\n<li>\n<p>속도가 빠르며 과적합 영향이 적다.</p>\n</li>\n<li>\n<p>적은 데이터셋이라도 준수한 결과를 도출한다.</p>\n</li>\n<li>\n<p>배깅의 대표적인 알고리즘 : RandomForest</p>\n</li>\n</ul>\n<h4 id=\"진행-과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\" aria-label=\"진행 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행 과정</h4>\n<p><img src=\"/5176a9f1d8def6595977e42df1978823/rf_3.png\" alt=\"rf_3.PNG\">\n[출처] <a href=\"https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f\">https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f</a></p>\n<h3 id=\"보팅\" style=\"position:relative;\"><a href=\"#%EB%B3%B4%ED%8C%85\" aria-label=\"보팅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>보팅</h3>\n<h4 id=\"보팅이란\" style=\"position:relative;\"><a href=\"#%EB%B3%B4%ED%8C%85%EC%9D%B4%EB%9E%80\" aria-label=\"보팅이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>보팅이란?</h4>\n<blockquote>\n<p>여러 분류기가 투표를 통해 예측 결과를 결정하는 방식</p>\n</blockquote>\n<h4 id=\"방식\" style=\"position:relative;\"><a href=\"#%EB%B0%A9%EC%8B%9D\" aria-label=\"방식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>방식</h4>\n<ul>\n<li>\n<p>소프트 보팅 :모든 분류기가 예측한 값의 결정 확률 평균을 구한 뒤 확률이 높은 값으로 결정</p>\n</li>\n<li>\n<p>하드 보팅 : 다수의 분류기가 예측한 값으로 결정</p>\n</li>\n</ul>\n<p><img src=\"/2c88b57bf183decb396a15286bbcc3b8/ens_2.png\" alt=\"ens_2.PNG\">\n[출처] <a href=\"https://velog.io/@gangjoo/ML-%EB%B6%84%EB%A5%98-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning%EA%B3%BC-%EB%B3%B4%ED%8C%85-Voting\">https://velog.io/@gangjoo/ML-%EB%B6%84%EB%A5%98-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning%EA%B3%BC-%EB%B3%B4%ED%8C%85-Voting</a></p>\n<h3 id=\"부스팅\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\" aria-label=\"부스팅 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스팅</h3>\n<h4 id=\"부스팅이란\" style=\"position:relative;\"><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85%EC%9D%B4%EB%9E%80\" aria-label=\"부스팅이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>부스팅이란?</h4>\n<blockquote>\n<p>부스팅은 가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법.</p>\n</blockquote>\n<p><img src=\"/24a7c14bcfbdc59571426527c9c16e8d/ens_4.png\" alt=\"ens_4.PNG\"></p>\n<h4 id=\"진행방식\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89%EB%B0%A9%EC%8B%9D\" aria-label=\"진행방식 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행방식</h4>\n<ol>\n<li>한 라운드 당 하나의 모델을 학습</li>\n<li>각 라운드 당 오분류된 객체들의 가중치를 조절</li>\n<li>조절된 가중치로 다시 학습</li>\n</ol>\n<p>위 1~3 과정을 반복하여 결과</p>\n<h4 id=\"특징-1\" style=\"position:relative;\"><a href=\"#%ED%8A%B9%EC%A7%95-1\" aria-label=\"특징 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>특징</h4>\n<ul>\n<li>\n<p>각 분류기가 순차적으로 진행</p>\n</li>\n<li>\n<p>데이터셋에 과적화될 위험성이 큼</p>\n</li>\n<li>\n<p>배깅 방식에 비해 속도가 느림</p>\n</li>\n<li>\n<p>결과 도출시에도 각 모델 결과에 가중치를 반영한다. 쉽게 말하면 나중 모델의 결과에 더 높은 가중치를 둠.</p>\n</li>\n<li>\n<p>대표적인 알고리즘으로 XGBoost, GBM, LightBoost 등이 있다.</p>\n</li>\n</ul>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"http://www.dinnopartners.com/__trashed-4/\">http://www.dinnopartners.com/__trashed-4/</a></p>\n<p>[2] <a href=\"https://nicola-ml.tistory.com/95\">https://nicola-ml.tistory.com/95</a></p>\n<p>[3] <a href=\"https://blog.naver.com/PostView.naver?blogId=hajuny2903&#x26;logNo=222422472569&#x26;redirect=Dlog&#x26;widgetTypeCall=true&#x26;directAccess=false\">https://blog.naver.com/PostView.naver?blogId=hajuny2903&#x26;logNo=222422472569&#x26;redirect=Dlog&#x26;widgetTypeCall=true&#x26;directAccess=false</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#ensemble\">Ensemble</a></p>\n<ul>\n<li>\n<p><a href=\"#%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-%EC%9C%A0%ED%98%95\">앙상블 학습 유형</a></p>\n</li>\n<li>\n<p><a href=\"#%EB%B0%B0%EA%B9%85bootstrap-aggregating-bagging\">배깅(Bootstrap Aggregating, Bagging)</a></p>\n<ul>\n<li><a href=\"#%EB%B0%B0%EA%B9%85%EC%9D%B4%EB%9E%80\">배깅이란?</a></li>\n<li><a href=\"#%ED%8A%B9%EC%A7%95\">특징</a></li>\n<li><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\">진행 과정</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B3%B4%ED%8C%85\">보팅</a></p>\n<ul>\n<li><a href=\"#%EB%B3%B4%ED%8C%85%EC%9D%B4%EB%9E%80\">보팅이란?</a></li>\n<li><a href=\"#%EB%B0%A9%EC%8B%9D\">방식</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85\">부스팅</a></p>\n<ul>\n<li><a href=\"#%EB%B6%80%EC%8A%A4%ED%8C%85%EC%9D%B4%EB%9E%80\">부스팅이란?</a></li>\n<li><a href=\"#%EC%A7%84%ED%96%89%EB%B0%A9%EC%8B%9D\">진행방식</a></li>\n<li><a href=\"#%ED%8A%B9%EC%A7%95-1\">특징</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"September 28, 2022","title":"Ensemble","categories":"ML","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/ML/[ML] ensemble/"}},"site":{"siteMetadata":{"siteUrl":"https://han-archives.github.io","comments":{"utterances":{"repo":"Han-Archives/han-archives.github.io"}}}}},"pageContext":{"slug":"/ML/[ML] Decision_Trees/","nextSlug":"/ML/[ML] Boosting_Algorithm/","prevSlug":"/ML/[ML] ensemble/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}