{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/DL/[DL] RNN/",
    "result": {"data":{"cur":{"id":"0dc6c46f-3d22-5ef4-a42d-f985c327be59","html":"<h2 id=\"rnn\" style=\"position:relative;\"><a href=\"#rnn\" aria-label=\"rnn permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RNN</h2>\n<p>이전 시점 정보(시계열 데이터)에 대해 적합한 인공신경망 모델</p>\n<h4 id=\"활용-분야\" style=\"position:relative;\"><a href=\"#%ED%99%9C%EC%9A%A9-%EB%B6%84%EC%95%BC\" aria-label=\"활용 분야 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>활용 분야</h4>\n<ul>\n<li>기계 번역</li>\n<li>음성 인식</li>\n<li>챗봇</li>\n<li>이미지/비디오 파악</li>\n<li>감성 분석</li>\n</ul>\n<h4 id=\"대표-알고리즘\" style=\"position:relative;\"><a href=\"#%EB%8C%80%ED%91%9C-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98\" aria-label=\"대표 알고리즘 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>대표 알고리즘</h4>\n<p>대표적인 RNN 알고리즘으로 RNN(Vanilla), LSTM, GRU 등이 있다.</p>\n<h3 id=\"진행-과정\" style=\"position:relative;\"><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\" aria-label=\"진행 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>진행 과정</h3>\n<p>RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖는다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAACLUlEQVQozyWRXUhTARiG3+ZMlmdLhG6CbrOEmEVEQUEURAy90SvBogIXaWPq3I/NNcvW3I9Nxe3sdHLq5k9zO87fYi2HZel20qa1NBTW0qQyCAZF3Z046716+S6e73v4wHEc5q41Im4LEcs2RjJ7owV8+HnonAycpResfVSy1jFB/LAMY8HKSJYtAfE3Uz9et43sTViDxAf7KN5aAzhwuhaYavMKU34WO/TEpbR7XPt56h3Py+WBP+e/5kSdU9h0BmtS5JhimZ7GlouRp6kJzdLDSaS7A3UpMlSz1h3AoqUvh8jPzx4jPHO1vbDBPH2xTu872Tq+JZI3e46Z+1eKrAMxnKifEd9yRMoUaqrkaJgTGruel2oMvccr3CmRsSNcptDQJQP++axRFsiXrrH1U53M+sf2wMZtKpI565n7+9gR/WUCx2Hgycrl+75VttWb1Kssw+X63tVPGveKVtUZOd/06H3KPLhqryyCCCDyCEKch4L9BbkABN4XhkMPQoqSHtafXcLnFftUyHdX+ObBe0NKacufPWjxNxw2DtZI1T2lMAwpj9iYJuk+I4DdEBAEIUDoi0XgS94Btaitot7oVJ6EDrM7lACFJtxlK3cFk1aQcW0tGVNfpxaaQcXV1fRSUz29aAAZa1SSMXU1HdMj+pvkgf+Vv3McIlw56EQ1/184wpuQj2yDywQw/3IDrrgMvrUrqJDZ0ccWYyajg+wCA+ezYjDbVXB5dVkjHvgPpX8GgEViUSYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"rnn_6.png\"\n        title=\"rnn_6.png\"\n        src=\"/static/4506af311185faaf6ea5e37dc1bc0964/37523/rnn_6.png\"\n        srcset=\"/static/4506af311185faaf6ea5e37dc1bc0964/e9ff0/rnn_6.png 180w,\n/static/4506af311185faaf6ea5e37dc1bc0964/f21e7/rnn_6.png 360w,\n/static/4506af311185faaf6ea5e37dc1bc0964/37523/rnn_6.png 720w,\n/static/4506af311185faaf6ea5e37dc1bc0964/302a4/rnn_6.png 1080w,\n/static/4506af311185faaf6ea5e37dc1bc0964/c1b63/rnn_6.png 1200w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n출처 : <a href=\"https://ko.wikipedia.org/wiki/%EC%88%9C%ED%99%98_%EC%8B%A0%EA%B2%BD%EB%A7%9D\">https://ko.wikipedia.org/wiki/%EC%88%9C%ED%99%98_%EC%8B%A0%EA%B2%BD%EB%A7%9D</a></p>\n<h4 id=\"예시\" style=\"position:relative;\"><a href=\"#%EC%98%88%EC%8B%9C\" aria-label=\"예시 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>예시</h4>\n<p><img src=\"/5c2ccd3021e4e86ea316ac88af1d8d06/rnn_4.png\" alt=\"rnn_4.PNG\"></p>\n<h3 id=\"종류\" style=\"position:relative;\"><a href=\"#%EC%A2%85%EB%A5%98\" aria-label=\"종류 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>종류</h3>\n<ul>\n<li>Many to One</li>\n</ul>\n<blockquote>\n<p>순차적인 x값들로 하나의 y값을 예측</p>\n</blockquote>\n<ul>\n<li>One to many</li>\n</ul>\n<blockquote>\n<p>하나의 시점(이미지) x로 순차적인 y 값들을 예측</p>\n</blockquote>\n<ul>\n<li>Many to many</li>\n</ul>\n<blockquote>\n<p>순차적인 X 값들로 순차적인 y 값들을 예측</p>\n</blockquote>\n<p><img src=\"/e390954adcf20d505c1655f3a8a44a1c/rnn_2.png\" alt=\"rnn_2.PNG\">\n출처: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf\">http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf</a></p>\n<h3 id=\"rnn-vanila-rnn-\" style=\"position:relative;\"><a href=\"#rnn-vanila-rnn-\" aria-label=\"rnn vanila rnn  permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RNN( Vanila RNN )</h3>\n<p><img src=\"/38cbbc7cd825845ceac0aed2e5c55782/rnn_3.png\" alt=\"rnn_3.PNG\"></p>\n<h3 id=\"lstm\" style=\"position:relative;\"><a href=\"#lstm\" aria-label=\"lstm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LSTM</h3>\n<p>기존 RNN 모델에 장기기억(long-term-memory)를 추가한 모델</p>\n<p><img src=\"/9f7211db34363b04a9400d44fb6c82eb/rnn_5.png\" alt=\"rnn_5.PNG\"></p>\n<h3 id=\"gru\" style=\"position:relative;\"><a href=\"#gru\" aria-label=\"gru permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GRU</h3>\n<p>LSTM 보다 간단한 구조지만 성능 면에서 밀리지 않는 RNN 모델</p>\n<p><img src=\"/02d85bd586ebbde56cefdecfe10ce42a/rnn_7.png\" alt=\"rnn_7.PNG\"></p>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf\">http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf</a></p>\n<p>[2] <a href=\"http://dprogrammer.org/rnn-lstm-gru\">http://dprogrammer.org/rnn-lstm-gru</a></p>\n<p>[3] <a href=\"https://www.youtube.com/watch?v=8HyCNIVRbSU\">https://www.youtube.com/watch?v=8HyCNIVRbSU</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#rnn\">RNN</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"#%ED%99%9C%EC%9A%A9-%EB%B6%84%EC%95%BC\">활용 분야</a></li>\n<li><a href=\"#%EB%8C%80%ED%91%9C-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98\">대표 알고리즘</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EC%A7%84%ED%96%89-%EA%B3%BC%EC%A0%95\">진행 과정</a></p>\n<ul>\n<li><a href=\"#%EC%98%88%EC%8B%9C\">예시</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EC%A2%85%EB%A5%98\">종류</a></p>\n</li>\n<li>\n<p><a href=\"#rnn-vanila-rnn-\">RNN( Vanila RNN )</a></p>\n</li>\n<li>\n<p><a href=\"#lstm\">LSTM</a></p>\n</li>\n<li>\n<p><a href=\"#gru\">GRU</a></p>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","excerpt":"RNN 이전 시점 정보(시계열 데이터)에 대해 적합한 인공신경망 모델 활용 분야 기계 번역 음성 인식 챗봇 이미지/비디오 파악 감성 분석 대표 알고리즘 대표적인 RNN 알고리즘으로 RNN(Vanilla), LSTM, GRU 등이 있다. 진행 과정 RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖는다. \n출처 : https://ko.wikipedia.org/wiki/%EC%88%9C%ED%99%98_%EC%8B%A0%EA%B2%BD%EB%A7%9D 예시 rnn_4.PNG 종류 Many to One 순차적인 x값들로 하나의 y값을 예측 One to many 하나의 시점(이미지) x로 순차적인 y 값들을 예측 Many to many 순차적인 X 값들로 순차적인 y 값들을 예측 rnn_2.PNG\n출처: http://cs231n.stanford.edu/slides/2017/cs231n_2017…","frontmatter":{"date":"December 19, 2022","title":"RNN","categories":"DL","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/DL/[DL] RNN/"}},"next":{"id":"be92d91d-e2d7-51b6-b994-278f13dfd7dd","html":"<h2 id=\"cnn-architecture\" style=\"position:relative;\"><a href=\"#cnn-architecture\" aria-label=\"cnn architecture permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>CNN Architecture</h2>\n<h3 id=\"소개\" style=\"position:relative;\"><a href=\"#%EC%86%8C%EA%B0%9C\" aria-label=\"소개 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>소개</h3>\n<p>CNN Architecture란 CNN을 기반으로 한 모델 중 성능이 뛰어난 몇몇 모델의 구조를 말한다.</p>\n<p>대표적인 아키텍처로 AlexNet, VGG, GoogleNet, ResNet 등이 있다.</p>\n<h3 id=\"발전-과정\" style=\"position:relative;\"><a href=\"#%EB%B0%9C%EC%A0%84-%EA%B3%BC%EC%A0%95\" aria-label=\"발전 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>발전 과정</h3>\n<p>ImageNet 데이터셋을 통한 년도별 성능 발전 상황을 살펴보면 2012년 AlexNet을 기점으로 CNN 구조의 여러 모델들이 높은 성능을 나타내며 발전하였다. 2016년도 이후로는 성능 향상보단 경량화, 효율화 과정을 통해 각 모델의 규모를 줄이는 데 초점을 맞춰 발전 해왔다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABOklEQVQoz41S2W4DIQzc//+2vkZ9qFIpR5PsmQALBuOjYkmqpG3UjlY8eGfMjHEjIgAgf4GZc87CXD9ezkZVS/UJ9IaMOBvTD8Ox63Z9f+i63dAXMSL+LhOhnBPRDLAbxtXQb6Zp3bYvH/vVZnM6nxvnXErpXlRdYs4ecT0Mb127M2br3NHamNI0joft9jxN5nJpjDEPtolVNSC+nk5djFm1xCNiIiHKOUOMF2MAABHLzcX2EoyZN+N4dO4dQq5Zb3aIWW4FXCAiTe0hRC6lEZMtIRmJVISfTJGI6lDKwJhob21rbcq5Nlf5F4q4RFF5tKn3Y/9RvDKvT6WqCTEEqC1CACKqKwDwvZhSqszmaw3qGi0/gvceQojMPhQAQIzROVcbVeaDuKKSnHPz7A2idc57Py+w1nrv78mf4Cq+5YEzqFgAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"cnnar_1.png\"\n        title=\"cnnar_1.png\"\n        src=\"/static/07d4b5b040679d2be33b1b37abd95d82/37523/cnnar_1.png\"\n        srcset=\"/static/07d4b5b040679d2be33b1b37abd95d82/e9ff0/cnnar_1.png 180w,\n/static/07d4b5b040679d2be33b1b37abd95d82/f21e7/cnnar_1.png 360w,\n/static/07d4b5b040679d2be33b1b37abd95d82/37523/cnnar_1.png 720w,\n/static/07d4b5b040679d2be33b1b37abd95d82/302a4/cnnar_1.png 1080w,\n/static/07d4b5b040679d2be33b1b37abd95d82/c1b63/cnnar_1.png 1200w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\n[출처] <a href=\"https://theaisummer.com/cnn-architectures/\">https://theaisummer.com/cnn-architectures/</a></p>\n<p>초반의 CNN 구조는 파라미터의 갯수를 증대시키는 방식으로 진행되었으나 현재 각 모델의 파라미터의 갯수와 성능은 아래의 표와 같이 언제나 비례하지 않는다.</p>\n<p><img src=\"/b43dd68dae65124854d7baf4f5d697a9/cnnar_5.png\" alt=\"cnnar_5.PNG\"></p>\n<h4 id=\"architecture-scaling\" style=\"position:relative;\"><a href=\"#architecture-scaling\" aria-label=\"architecture scaling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture Scaling</h4>\n<p>각 모델 구조는 아래 그림과 같이 넓게, 깊게, 넓고 깊게하는 방식으로 스케일링을 진행.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 55.55555555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABlUlEQVQoz3VRS4/TMBDOX+fHcEFw5cIJVgjtBWlZBFuo0jZpHNsZv+3YieMYOSkrIbQjnzzfa2aqdav8Qi3zBMff5OmH4yxtPwX8F1/9g90bz2+DcvxraB/myf3vUHnv3ehSSi+ZcziT/mme/JrzMgV5OavLeXamkPv2eq1Po7U55zhPwVjAhBEaRz9qnXPuGT6iY9qCpBQtHM1wSHEqZMd6TepdKcWoaatwzbujxicnWc55ELKhg9Fam4IB1tChXteStDIGAC7LppRz9qOQsuOsNRrP2+f9T/7mI0rzaExJR2SPFdpXVikrqUBLWm7k4IlAIBEoSKlAvtXi/X1XhlqK26dH/vYObyOsVUPcu7sujNY6t/M/fKWvXh+UvWVxzktRAsdYDB5r8fk7FPK6VtqGwwmsNdbaZdNusf7ygOZ4y2K0Y6DKzbeut16BIIR4H6oYFwEcACilQojSdoFjsV895xz8pKR9dnYuMKa67joMUDHGm6YJIew+0zRhjK9dx6Cs2nvfI4RQv+s653qEKKGMsRDCHxj7dUzQUmjeAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"cnnar_3.png\"\n        title=\"cnnar_3.png\"\n        src=\"/static/cfd4539de426a2a7ea8d6c6b1ef4db08/37523/cnnar_3.png\"\n        srcset=\"/static/cfd4539de426a2a7ea8d6c6b1ef4db08/e9ff0/cnnar_3.png 180w,\n/static/cfd4539de426a2a7ea8d6c6b1ef4db08/f21e7/cnnar_3.png 360w,\n/static/cfd4539de426a2a7ea8d6c6b1ef4db08/37523/cnnar_3.png 720w,\n/static/cfd4539de426a2a7ea8d6c6b1ef4db08/5205c/cnnar_3.png 833w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>[출처] <a href=\"https://theaisummer.com/cnn-architectures/\">https://theaisummer.com/cnn-architectures/</a></p>\n<h3 id=\"alexnet\" style=\"position:relative;\"><a href=\"#alexnet\" aria-label=\"alexnet permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AlexNet</h3>\n<p>AlexNet은 2012년 ImageNet의 분류 대회에서 63% 정확도로 우승한 모델로 5개의 Conv 층과 3개의 Full Connect 층으로 구성</p>\n<p><img src=\"/5e7bc6d8926d90d027782c46f68c1027/cnnar_7.png\" alt=\"cnnar_7.PNG\">\n[출처]<a href=\"http://cs231n.stanford.edu/slides/2017/\">http://cs231n.stanford.edu/slides/2017/</a></p>\n<h4 id=\"주요-특징\" style=\"position:relative;\"><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8A%B9%EC%A7%95\" aria-label=\"주요 특징 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>주요 특징</h4>\n<ul>\n<li>input 값을 256x256으로 resize</li>\n<li>tanh 대신 처음으로 RELU 활성화 함수 사용</li>\n<li>Norm layer 사용</li>\n<li>Data Augmentation을 통해 데이터셋 크기를 2048배 확장</li>\n<li>0.5의 확률로 뉴런이 네트워크에서 삭제. (드롭아웃)</li>\n</ul>\n<h4 id=\"구조\" style=\"position:relative;\"><a href=\"#%EA%B5%AC%EC%A1%B0\" aria-label=\"구조 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>구조</h4>\n<p><img src=\"/b6a85f9b1172f3d73e4037a28bca468e/cnnar_6.png\" alt=\"cnnar_6.PNG\"></p>\n<h3 id=\"vgg\" style=\"position:relative;\"><a href=\"#vgg\" aria-label=\"vgg permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>VGG</h3>\n<p>VGG(Visual Geometry Group)는 다중 레이어가 있는 심층 CNN 구조로 대표적인 모델로 VGG-16, VGG-19가 있다. (VGG 뒤 번호는 각 모델의 layer의 수)</p>\n<p><img src=\"/1ac570a9f34197ae95fd893346b090d8/cnnar_11.png\" alt=\"cnnar_11.PNG\">\nVGG-16의 경우 13개의 Conv layer와 3개의 Full Connect Layer로 구성</p>\n<h4 id=\"주요-특징-1\" style=\"position:relative;\"><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8A%B9%EC%A7%95-1\" aria-label=\"주요 특징 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>주요 특징</h4>\n<ul>\n<li>모델 입력값은 224x224</li>\n<li>3x3 커널만을 사용. 학습 진행시 kernels의 수가 2배씩 증가.</li>\n<li>엄청난 파라미터 수로 인해 학습 많은 학습 시간이 소요됨.</li>\n<li>다른 모델에 비해 다량의 메모리가 필요.</li>\n</ul>\n<h4 id=\"구조-1\" style=\"position:relative;\"><a href=\"#%EA%B5%AC%EC%A1%B0-1\" aria-label=\"구조 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>구조</h4>\n<p><img src=\"/bcb952b85e90d747b48f107db61fa48c/cnnar_12.png\" alt=\"cnnar_12.PNG\">\n[출처] <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf\">http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf</a></p>\n<h3 id=\"googlenet--inceptionnet\" style=\"position:relative;\"><a href=\"#googlenet--inceptionnet\" aria-label=\"googlenet  inceptionnet permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GoogleNet / InceptionNet</h3>\n<p>계산 효율성을 높인 더 깊은 네트워크.</p>\n<p>1x1, 3x3, 5x5 다양한 필터 사이즈를 사용한 네트워크.</p>\n<h4 id=\"inception-구조\" style=\"position:relative;\"><a href=\"#inception-%EA%B5%AC%EC%A1%B0\" aria-label=\"inception 구조 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Inception 구조</h4>\n<p>GoogleNet은 Inception이라는 구조를 사용. Inception이란 1x1, 3x3, 5x5 Convolutions 를 결합한 module로 1x1 Convolutions을 통해 파라미터의 수를 축소 시킨 다음 3x3, 5x5 필터를 통해 진행</p>\n<p><img src=\"/c06b3ff6f941dd4ebbc23ae5cf7579be/cnnar_13.png\" alt=\"cnnar_13.PNG\"></p>\n<h4 id=\"다양한-필터-사이즈의-이점\" style=\"position:relative;\"><a href=\"#%EB%8B%A4%EC%96%91%ED%95%9C-%ED%95%84%ED%84%B0-%EC%82%AC%EC%9D%B4%EC%A6%88%EC%9D%98-%EC%9D%B4%EC%A0%90\" aria-label=\"다양한 필터 사이즈의 이점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>다양한 필터 사이즈의 이점</h4>\n<p><img src=\"/46637ea50599b4fa2ee839c8b98bac1a/cnnar_14.png\" alt=\"cnnar_14.PNG\"></p>\n<p>이미지에서 특징적인 포인트는 이미지 속 객체의 크기에 따라 큰 편차가 발생. 사이즈가 큰 필터를 사용하면 그림 2의 특징적인 부분이 그림 1에 비해 잘 파악하지 못하게 되며 이를 방지하기 위해 1x1, 3x3, 5x5와 같이 다양한 크기의 필터를 활용</p>\n<h4 id=\"주요-특징-2\" style=\"position:relative;\"><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8A%B9%EC%A7%95-2\" aria-label=\"주요 특징 2 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>주요 특징</h4>\n<ul>\n<li>22 layers으로 구성</li>\n<li>Inception Module을 사용</li>\n<li>Full Connect layer를 사용하지 않음</li>\n<li>AlexNet 보다 12배 적은 파라미터</li>\n<li>다양한 크기의 필터 활용</li>\n</ul>\n<h4 id=\"구조-2\" style=\"position:relative;\"><a href=\"#%EA%B5%AC%EC%A1%B0-2\" aria-label=\"구조 2 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>구조</h4>\n<p><img src=\"/5460979b90a0079c7d95cfd64c75dbfd/cnnar_15.png\" alt=\"cnnar_15.PNG\"></p>\n<p>※ 보조 분류기란 심층 네트워크의 수렴을 개선하기 위해 사용.</p>\n<h3 id=\"resnet\" style=\"position:relative;\"><a href=\"#resnet\" aria-label=\"resnet permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ResNet</h3>\n<p>잔차 신경망(ResNet)은 Skip Connection을 통해 잔차를 학습하는 신경망. 152개의 layers으로 구성</p>\n<h4 id=\"기울기-소실\" style=\"position:relative;\"><a href=\"#%EA%B8%B0%EC%9A%B8%EA%B8%B0-%EC%86%8C%EC%8B%A4\" aria-label=\"기울기 소실 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>기울기 소실</h4>\n<p>앞서 살펴본 AlexNet, VGG, GoogleNet을 보면 년도가 지날 수록 layer 층이 더 깊어지며 발전되어왔다. 심층 네트워크 모델의 깊이가 증가할 수록 복잡한 기능을 구현되어 모델의 성능이 향상된다. 하지만 더 많은 layers를 쌓는 과정에서 기울기 소실 문제가 발생되어 더 많은 layers 쌓아도 성능이 저하되는 문제가 발생한다.</p>\n<h4 id=\"잔차-블록\" style=\"position:relative;\"><a href=\"#%EC%9E%94%EC%B0%A8-%EB%B8%94%EB%A1%9D\" aria-label=\"잔차 블록 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>잔차 블록</h4>\n<p>심층 네트워크의 기울기 소실 문제를 잔차블록을 통해 보완했다. 잔차블록은 기존의 망에 입력 값을 출력값에 더해주는 길을 추가한 것.</p>\n<p>잔차블록의 핵심 아이디어는 기존의 방식의 결과값 H(x)와 입력값 x의 차이 F(x)를 0에 근사하도록 학습을 진행하는 방향으로 진행.</p>\n<p><img src=\"/da569266b22053106ba60dc874bb5247/cnnar_16.png\" alt=\"cnnar_16.PNG\"></p>\n<h4 id=\"주요-특징-3\" style=\"position:relative;\"><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8A%B9%EC%A7%95-3\" aria-label=\"주요 특징 3 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>주요 특징</h4>\n<ul>\n<li>잔차 블록 결합 층으로 구성</li>\n<li>주기적으로 필터 수를 2배로 증가, stride 2를 적용하여 다운샘플링 진행</li>\n<li>ResNet 50미만의 모델들은 3x3 Conv layers를 사용하지만 ResNet50 이상의 모델의 경우 계산 효율을 높이기 위해 bottleneck. 즉 1x1 Conv를 활용</li>\n</ul>\n<p><img src=\"/f5d307d47dce7289a2bcd2ec2dd06b5e/cnnar_17.png\" alt=\"cnnar_17.PNG\">\n[출처] <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf\">http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf</a></p>\n<h4 id=\"구성\" style=\"position:relative;\"><a href=\"#%EA%B5%AC%EC%84%B1\" aria-label=\"구성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>구성</h4>\n<p><img src=\"/c73f223d549a55b4ef09183505cee7c9/cnnar_18.jfif\" alt=\"cnnar_18.jfif\"></p>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<p>[1] <a href=\"https://www.youtube.com/watch?v=4AuCBqPvmcc&#x26;list=WL&#x26;index=3&#x26;t=131s\">https://www.youtube.com/watch?v=4AuCBqPvmcc&#x26;list=WL&#x26;index=3&#x26;t=131s</a></p>\n<p>[2] <a href=\"https://wikidocs.net/164335\">https://wikidocs.net/164335</a></p>\n<p>[3] <a href=\"https://theaisummer.com/cnn-architectures/\">https://theaisummer.com/cnn-architectures/</a></p>\n<p>[4] <a href=\"http://cs231n.stanford.edu/slides/2017/\">http://cs231n.stanford.edu/slides/2017/</a></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#cnn-architecture\">CNN Architecture</a></p>\n<ul>\n<li>\n<p><a href=\"#%EC%86%8C%EA%B0%9C\">소개</a></p>\n</li>\n<li>\n<p><a href=\"#%EB%B0%9C%EC%A0%84-%EA%B3%BC%EC%A0%95\">발전 과정</a></p>\n<ul>\n<li><a href=\"#architecture-scaling\">Architecture Scaling</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#alexnet\">AlexNet</a></p>\n<ul>\n<li><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8A%B9%EC%A7%95\">주요 특징</a></li>\n<li><a href=\"#%EA%B5%AC%EC%A1%B0\">구조</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#vgg\">VGG</a></p>\n<ul>\n<li><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8A%B9%EC%A7%95-1\">주요 특징</a></li>\n<li><a href=\"#%EA%B5%AC%EC%A1%B0-1\">구조</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#googlenet--inceptionnet\">GoogleNet / InceptionNet</a></p>\n<ul>\n<li><a href=\"#inception-%EA%B5%AC%EC%A1%B0\">Inception 구조</a></li>\n<li><a href=\"#%EB%8B%A4%EC%96%91%ED%95%9C-%ED%95%84%ED%84%B0-%EC%82%AC%EC%9D%B4%EC%A6%88%EC%9D%98-%EC%9D%B4%EC%A0%90\">다양한 필터 사이즈의 이점</a></li>\n<li><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8A%B9%EC%A7%95-2\">주요 특징</a></li>\n<li><a href=\"#%EA%B5%AC%EC%A1%B0-2\">구조</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#resnet\">ResNet</a></p>\n<ul>\n<li><a href=\"#%EA%B8%B0%EC%9A%B8%EA%B8%B0-%EC%86%8C%EC%8B%A4\">기울기 소실</a></li>\n<li><a href=\"#%EC%9E%94%EC%B0%A8-%EB%B8%94%EB%A1%9D\">잔차 블록</a></li>\n<li><a href=\"#%EC%A3%BC%EC%9A%94-%ED%8A%B9%EC%A7%95-3\">주요 특징</a></li>\n<li><a href=\"#%EA%B5%AC%EC%84%B1\">구성</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#reference\">Reference</a></p>\n</li>\n</ul>\n</li>\n</ul>\n</div>","frontmatter":{"date":"December 08, 2022","title":"CNN Architecture","categories":"DL","author":"HwanHee Park","emoji":"📝"},"fields":{"slug":"/DL/[DL] CNN_Architectures/"}},"prev":null,"site":{"siteMetadata":{"siteUrl":"https://han-archives.github.io","comments":{"utterances":{"repo":"Han-Archives/han-archives.github.io"}}}}},"pageContext":{"slug":"/DL/[DL] RNN/","nextSlug":"/DL/[DL] CNN_Architectures/","prevSlug":""}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}